{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenue sur le Blog Technique de Neutron IT","text":"<p>Neutron IT est fi\u00e8re de vous pr\u00e9senter son blog d\u00e9di\u00e9 aux articles techniques sur les technologies DevOps et cloud. En tant que sp\u00e9cialistes OpenShift, nous partageons notre expertise sur une vari\u00e9t\u00e9 de sujets pour aider les entreprises \u00e0 optimiser leurs infrastructures et processus de d\u00e9veloppement.</p>"},{"location":"#a-propos-de-neutron-it","title":"\u00c0 Propos de Neutron IT","text":"<p>Neutron IT est une entreprise leader dans le domaine des technologies DevOps et cloud. Notre mission est de fournir des solutions innovantes et des conseils d'experts pour aider nos clients \u00e0 atteindre leurs objectifs technologiques.</p>"},{"location":"#contact","title":"Contact","text":"<p>Pour toute question ou suggestion, n'h\u00e9sitez pas \u00e0 nous contacter via notre site web.</p>"},{"location":"#formation","title":"Formation","text":"<p>Pour nos formation openshift nos formations</p>"},{"location":"BootC/MinIO/","title":"D\u00e9ployer MinIO avec BootC","text":"","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#deployer-minio-avec-bootc","title":"D\u00e9ployer MinIO avec BootC","text":"","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#1-introduction","title":"1. Introduction","text":"","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#presentation-de-minio-et-bootc","title":"Pr\u00e9sentation de MinIO et BootC","text":"<p>MinIO est une solution de stockage d'objets open-source qui permet de cr\u00e9er des infrastructures de stockage \u00e9volutives et r\u00e9silientes. BootC, quant \u00e0 lui, est un outil de gestion de conteneurs qui facilite le d\u00e9ploiement et la gestion des applications conteneuris\u00e9es.</p>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#importance-du-stockage-dobjets-et-des-conteneurs","title":"Importance du stockage d'objets et des conteneurs","text":"<p>Le stockage d'objets est essentiel pour g\u00e9rer de grandes quantit\u00e9s de donn\u00e9es non structur\u00e9es, tandis que les conteneurs permettent de d\u00e9ployer des applications de mani\u00e8re coh\u00e9rente et isol\u00e9e. La combinaison de MinIO et BootC offre une solution puissante pour le stockage et la gestion des donn\u00e9es dans des environnements modernes.</p>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#2-quest-ce-que-minio","title":"2. Qu'est-ce que MinIO ?","text":"","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#description-de-minio","title":"Description de MinIO","text":"<p>MinIO est un serveur de stockage d'objets compatible avec le protocole Amazon S3. Il est con\u00e7u pour \u00eatre l\u00e9ger, rapide et facile \u00e0 d\u00e9ployer. MinIO peut \u00eatre utilis\u00e9 pour stocker des photos, des vid\u00e9os, des sauvegardes et d'autres types de donn\u00e9es non structur\u00e9es.</p>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#avantages-et-caracteristiques-principales","title":"Avantages et caract\u00e9ristiques principales","text":"<ul> <li>Compatibilit\u00e9 S3 : MinIO est enti\u00e8rement compatible avec le protocole Amazon S3, ce qui permet une int\u00e9gration facile avec les outils et applications existants.</li> <li>Haute performance : MinIO est optimis\u00e9 pour offrir des performances \u00e9lev\u00e9es en termes de d\u00e9bit et de latence.</li> <li>\u00c9volutivit\u00e9 : MinIO peut \u00eatre d\u00e9ploy\u00e9 sur un seul n\u0153ud ou sur plusieurs n\u0153uds pour r\u00e9pondre aux besoins de stockage croissants.</li> <li>S\u00e9curit\u00e9 : MinIO offre des fonctionnalit\u00e9s de s\u00e9curit\u00e9 robustes, telles que le chiffrement des donn\u00e9es et l'authentification.</li> </ul>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#3-quest-ce-que-bootc","title":"3. Qu'est-ce que BootC ?","text":"","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#description-de-bootc","title":"Description de BootC","text":"<p>BootC est un outil de gestion de conteneurs qui simplifie le d\u00e9ploiement, la gestion et l'orchestration des applications conteneuris\u00e9es. Il est con\u00e7u pour \u00eatre l\u00e9ger et facile \u00e0 utiliser, tout en offrant des fonctionnalit\u00e9s puissantes pour les environnements de production. BootC est au c\u0153ur des conteneurs amor\u00e7ables, qui permettent de g\u00e9rer des syst\u00e8mes d'exploitation immuables via des images de conteneurs OCI/Docker.</p>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#avantages-et-caracteristiques-principales_1","title":"Avantages et caract\u00e9ristiques principales","text":"<ul> <li>Approche unifi\u00e9e pour DevOps : BootC permet de g\u00e9rer l'ensemble du syst\u00e8me d'exploitation via des outils et concepts bas\u00e9s sur les conteneurs, y compris GitOps et CI/CD.</li> <li>S\u00e9curit\u00e9 simplifi\u00e9e : Les mises \u00e0 jour de s\u00e9curit\u00e9 peuvent \u00eatre appliqu\u00e9es \u00e0 l'ensemble du syst\u00e8me d'exploitation, y compris le noyau, les pilotes et le chargeur d'amor\u00e7age, en utilisant des outils de s\u00e9curit\u00e9 avanc\u00e9s pour les conteneurs.</li> <li>Int\u00e9gration rapide et \u00e9cosyst\u00e8me : BootC s'int\u00e8gre dans un vaste \u00e9cosyst\u00e8me d'outils et de technologies autour des conteneurs, permettant de d\u00e9ployer et de g\u00e9rer des syst\u00e8mes Linux \u00e0 grande \u00e9chelle.</li> </ul>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#4-prerequis","title":"4. Pr\u00e9requis","text":"","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#liste-des-elements-necessaires-avant-de-commencer","title":"Liste des \u00e9l\u00e9ments n\u00e9cessaires avant de commencer","text":"<ul> <li>Podman install\u00e9 sur votre syst\u00e8me</li> <li>Espace disque suffisant pour le stockage des conteneurs et des fichiers de sortie</li> </ul>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#5-etapes-de-deploiement","title":"5. \u00c9tapes de d\u00e9ploiement","text":"","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#etape-1-creer-le-containerfile","title":"\u00c9tape 1 : Cr\u00e9er le <code>Containerfile</code>","text":"<p>Adaptez le <code>Containerfile</code> suivant \u00e0 vos besoins :</p> <pre><code>FROM quay.io/centos/centos:stream9 as builder\n\nRUN dnf install -y wget\nRUN wget https://dl.min.io/server/minio/release/linux-amd64/minio\n\nFROM quay.io/centos-bootc/centos-bootc:stream9\n\nCOPY --from=builder --chmod=755 /minio /usr/local/bin/minio\n\nRUN cat &lt;&lt;'EOF' &gt; /etc/systemd/system/minio.service\n[Unit]\nDescription=MinIO\nDocumentation=https://min.io/docs/minio/linux/index.html\nWants=network-online.target\nAfter=network-online.target\nAssertFileIsExecutable=/usr/local/bin/minio\n\n[Service]\nWorkingDirectory=/usr/local\n\nUser=minio\nGroup=minio\nProtectProc=invisible\n\nEnvironmentFile=-/etc/default/minio\nExecStartPre=/bin/bash -c \"if [ -z \\\"$MINIO_VOLUMES\\\" ]; then echo \\\"Variable MINIO_VOLUMES not set in /etc/default/minio\\\"; exit 1; fi\"\nExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES\n\n# MinIO RELEASE.2023-05-04T21-44-30Z adds support for Type=notify (https://www.freedesktop.org/software/systemd/man/systemd.service.html#Type=)\n# This may improve systemctl setups where other services use `After=minio.server`\n# Uncomment the line to enable the functionality\n# Type=notify\n\n# Let systemd restart this service always\nRestart=always\n\n# Specifies the maximum file descriptor number that can be opened by this process\nLimitNOFILE=65536\n\n# Specifies the maximum number of threads this process can create\nTasksMax=infinity\n\n# Disable timeout logic and wait until process is stopped\nTimeoutStopSec=infinity\nSendSIGKILL=no\n\n[Install]\nWantedBy=multi-user.target\n\nEOF\n\nRUN cat &lt;&lt;'EOF' &gt; /etc/default/minio\n# MINIO_ROOT_USER and MINIO_ROOT_PASSWORD sets the root account for the MinIO server.\n# This user has unrestricted permissions to perform S3 and administrative API operations on any resource in the deployment.\n# Omit to use the default values 'minioadmin:minioadmin'.\n# MinIO recommends setting non-default values as a best practice, regardless of environment\n\nMINIO_ROOT_USER=minioadmin\nMINIO_ROOT_PASSWORD=minioadmin\n\n# MINIO_VOLUMES sets the storage volume or path to use for the MinIO server.\n\nMINIO_VOLUMES=\"/home/minio\"\n\n# MINIO_OPTS sets any additional commandline options to pass to the MinIO server.\n# For example, `--console-address :9001` sets the MinIO Console listen port\nMINIO_OPTS=\"--console-address :9001\"\nEOF\n\nWORKDIR /home\n\nRUN mkdir minio\n\nRUN systemctl enable minio.service\n</code></pre>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#etape-2-creer-le-fichier-configtoml","title":"\u00c9tape 2 : Cr\u00e9er le fichier <code>config.toml</code>","text":"<p>Adaptez le fichier <code>config.toml</code> suivant \u00e0 vos besoins : </p> <pre><code>[customizations.installer.kickstart]\ncontents = \"\"\"\n# Non-interactive text mode installation\ntext --non-interactive\nreboot --eject\n\n# Set language and keyboard layout\nlang en_US\nkeyboard --vckeymap=fr --xlayouts='fr'\n\n# Network configuration\nnetwork --bootproto=dhcp --device=link --activate --onboot=on\n\nzerombr\nclearpart --all --initlabel --disklabel=gpt\nautopart --noswap --type=lvm --fstype=xfs \n\n# Create a user with SSH key\nuser --name=minio --password=toto --shell=/bin/bash --groups=wheel\n\n%post\n# Configure sudo for the user\necho \"minio ALL=(ALL) NOPASSWD: ALL\" &gt; /etc/sudoers.d/minio\nchmod 440 /etc/sudoers.d/minio\n%end\n\"\"\"\n</code></pre>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#etape-3-construire-limage-bootc-locale-pour-minio","title":"\u00c9tape 3 : Construire l'image Bootc locale pour MinIO","text":"<ol> <li>D\u00e9finissez vos identifiants Red Hat et le nom de l'image :</li> </ol> <p><code>bash    export IMAGE_NAME=minio-centos-bootc</code></p> <ol> <li>Ex\u00e9cutez la commande <code>podman build</code> pour cr\u00e9er l'image Bootc pour MinIO :</li> </ol> <p><code>bash    sudo podman build -t \"${IMAGE_NAME}\" -f Containerfile</code></p> <p>Remarque : Le <code>Containerfile</code> doit \u00eatre pr\u00e9sent dans le r\u00e9pertoire de travail.</p>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#etape-4-creer-liso-pour-minio","title":"\u00c9tape 4 : Cr\u00e9er l'ISO pour MinIO","text":"<ol> <li>D\u00e9finissez les variables n\u00e9cessaires :</li> </ol> <p><code>bash    export CONFIG_PATH=&lt;votre_chemin&gt;/config.toml    export OUTPUT_DIR=&lt;chemin_vers_r\u00e9pertoire_de_sortie&gt;</code></p> <ol> <li>Ex\u00e9cutez la commande <code>podman run</code> pour g\u00e9n\u00e9rer l'ISO :</li> </ol> <p><code>bash    sudo podman run \\        --rm \\        -it \\        --privileged \\        --pull=newer \\        --security-opt label=type:unconfined_t \\        -v ${CONFIG_PATH}:/config.toml:ro \\        -v ${OUTPUT_DIR}:/output \\        -v /var/lib/containers/storage:/var/lib/containers/storage \\        quay.io/centos-bootc/bootc-image-builder \\        --type iso \\        --use-librepo \\        --local localhost/${IMAGE_NAME}</code></p> <p>Remarques :    - Assurez-vous que <code>config.toml</code> est correctement configur\u00e9 pour votre installation MinIO.    - Le r\u00e9pertoire de sortie doit \u00eatre accessible en \u00e9criture pour stocker l'ISO r\u00e9sultant.    - L'option <code>--pull=newer</code> garantit l'utilisation de la derni\u00e8re image <code>bootc-image-builder</code>.    - L'option <code>--security-opt label=type:unconfined_t</code> aide \u00e0 \u00e9viter les probl\u00e8mes li\u00e9s \u00e0 SELinux.</p>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#etape-5-installer-centos-stream-9-avec-minio","title":"\u00c9tape 5 : Installer CentOS Stream 9 avec MinIO","text":"<ol> <li> <p>Cr\u00e9er un support d'installation amor\u00e7able : Utilisez l'ISO g\u00e9n\u00e9r\u00e9 pour cr\u00e9er un support d'installation amor\u00e7able (par exemple, une cl\u00e9 USB) \u00e0 l'aide d'un outil comme Rufus (Windows) ou dd (Linux).</p> </li> <li> <p>D\u00e9marrer \u00e0 partir du support : Ins\u00e9rez le support d'installation dans votre syst\u00e8me cible et d\u00e9marrez \u00e0 partir de celui-ci.</p> </li> <li> <p>Suivre les instructions d'installation : Proc\u00e9dez \u00e0 l'installation en suivant les instructions \u00e0 l'\u00e9cran.</p> </li> <li> <p>V\u00e9rifier l'installation : Une fois l'installation termin\u00e9e, v\u00e9rifiez que MinIO est en cours d'ex\u00e9cution en contr\u00f4lant l'\u00e9tat du service :</p> </li> </ol> <p><code>bash    sudo systemctl status MinIO</code></p>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#6-configuration-avancee-optionnel","title":"6. Configuration avanc\u00e9e (optionnel)","text":"","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#options-de-configuration-supplementaires","title":"Options de configuration suppl\u00e9mentaires","text":"<ul> <li>R\u00e9plication : Configurez la r\u00e9plication des donn\u00e9es entre plusieurs n\u0153uds MinIO pour une meilleure r\u00e9silience.</li> <li>S\u00e9curit\u00e9 : Activez le chiffrement des donn\u00e9es et configurez les politiques de s\u00e9curit\u00e9 pour prot\u00e9ger vos donn\u00e9es.</li> </ul>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#securite-et-optimisation","title":"S\u00e9curit\u00e9 et optimisation","text":"<ul> <li>Sauvegarde : Mettez en place des sauvegardes r\u00e9guli\u00e8res pour prot\u00e9ger vos donn\u00e9es contre les pertes.</li> <li>Surveillance : Utilisez des outils de surveillance pour suivre les performances et la sant\u00e9 de votre d\u00e9ploiement MinIO.</li> </ul>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#7-conclusion","title":"7. Conclusion","text":"","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#recapitulatif-des-avantages-de-minio-et-bootc","title":"R\u00e9capitulatif des avantages de MinIO et BootC","text":"<p>MinIO et BootC offrent une solution puissante et flexible pour le stockage et la gestion des donn\u00e9es dans des environnements modernes. La combinaison de ces deux outils permet de cr\u00e9er des infrastructures de stockage \u00e9volutives, s\u00e9curis\u00e9es et faciles \u00e0 g\u00e9rer.</p>","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#8-ressources-supplementaires","title":"8. Ressources suppl\u00e9mentaires","text":"","tags":["BootC","MinIO","Storage"]},{"location":"BootC/MinIO/#liens-vers-les-documentations","title":"Liens vers les documentations","text":"<ul> <li>Documentation de MinIO</li> <li>Documentation de BootC</li> <li>Documentation d'OSBuild</li> </ul> <p>Auteur : Romain GASQUET</p>","tags":["BootC","MinIO","Storage"]},{"location":"OpenShift/Configuration/openshift-image-registry/","title":"Exposition de la registry interne sur OpenShift","text":"","tags":["OpenShift","Configuration","Registry"]},{"location":"OpenShift/Configuration/openshift-image-registry/#prerequis","title":"Pr\u00e9requis","text":"<p>Avant de commencer, assurez-vous de remplir les conditions suivantes :</p> <ul> <li>\u00catre en mesure de se connecter \u00e0 un cluster OpenShift.</li> <li>Avoir les privil\u00e8ges <code>cluster-admin</code>.</li> </ul>","tags":["OpenShift","Configuration","Registry"]},{"location":"OpenShift/Configuration/openshift-image-registry/#1-creer-un-pvc","title":"1. Cr\u00e9er un PVC","text":"<p>Pour commencer, cr\u00e9ez un PVC d\u00e9di\u00e9 \u00e0 la registry interne sur OpenShift :</p>","tags":["OpenShift","Configuration","Registry"]},{"location":"OpenShift/Configuration/openshift-image-registry/#si-vous-avez-openshift-data-foundation-conseille","title":"Si vous avez OpenShift Data Foundation (conseill\u00e9) :","text":"<p>Cr\u00e9ez le fichier <code>image-registry-storage.yml</code> :</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: image-registry-storage\n  namespace: openshift-image-registry\nspec:\n  storageClassName: &lt;your_odf_fs_storage_class&gt;\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 100Gi\n</code></pre> <p>Cr\u00e9ez la ressource OpenShift :</p> <pre><code>oc apply -f image-registry-storage.yml\n</code></pre>","tags":["OpenShift","Configuration","Registry"]},{"location":"OpenShift/Configuration/openshift-image-registry/#si-vous-avez-du-stockage-local-sno","title":"Si vous avez du stockage local (SNO) :","text":"<p>Cr\u00e9ez le fichier <code>image-registry-storage.yml</code> :</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: image-registry-storage\n  namespace: openshift-image-registry\nspec:\n  storageClassName: &lt;your_local_storage_class&gt;\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Gi\n  volumeMode: Filesystem\n</code></pre> <p>Cr\u00e9ez la ressource OpenShift :</p> <pre><code>oc apply -f image-registry-storage.yml\n</code></pre>","tags":["OpenShift","Configuration","Registry"]},{"location":"OpenShift/Configuration/openshift-image-registry/#2-modifier-la-configuration-de-loperateur","title":"2. Modifier la configuration de l'Op\u00e9rateur","text":"<p>Editez la configuration de l'Op\u00e9rateur <code>image registry</code>:</p> <pre><code>oc edit configs.imageregistry.operator.openshift.io -n openshift-image-registry\n</code></pre> <p>Modifiez les attributs suivants : </p> <pre><code>spec:\n  defaultRoute: true\n  managementState: Managed\n  storage:\n    pvc:\n      claim: image-registry-storage\n  rolloutStrategy: Recreate # Uniquement si on utilise du stockage RWO pour \u00e9viter les probl\u00e8mes de montage\n</code></pre> <p>Attendez que l'Op\u00e9rateur soit \u00e0 nouveau disponible :</p> <pre><code>watch oc get co image-registry\n</code></pre>","tags":["OpenShift","Configuration","Registry"]},{"location":"OpenShift/Configuration/openshift-image-registry/#3-pousser-une-image-sur-la-registry","title":"3. Pousser une image sur la registry","text":"<p>R\u00e9cup\u00e9rez la route de la registry :</p> <pre><code>HOST=$(oc get route default-route -n openshift-image-registry --template='{{ .spec.host }}')\n</code></pre> <p>Ajoutez les certificats de la registry :</p> <pre><code>oc extract secret/$(oc get ingresscontroller -n openshift-ingress-operator default -o json | jq '.spec.defaultCertificate.name // \"router-certs-default\"' -r) -n openshift-ingress --confirm\n</code></pre> <pre><code>sudo mv tls.crt /etc/pki/ca-trust/source/anchors/\n</code></pre> <pre><code>sudo update-ca-trust enable\n</code></pre> <p>Connectez vous \u00e0 la registry :</p> <pre><code>sudo podman login -u &lt;user&gt; -p $(oc whoami -t) $HOST\n</code></pre> <p>Taguez l'image que vous souahitez pousser : </p> <pre><code>podman tag &lt;your_image&gt; $HOST/&lt;your_project&gt;/&lt;your_image&gt;\n</code></pre> <p>Cr\u00e9ez le namespace associ\u00e9 \u00e0 votre projet :</p> <pre><code>oc new-project &lt;your_project&gt;\n</code></pre> <p>Poussez votre image sur la registry :</p> <pre><code>podman push $HOST/&lt;your_project&gt;/&lt;your_image&gt;\n</code></pre>","tags":["OpenShift","Configuration","Registry"]},{"location":"OpenShift/Configuration/openshift-image-registry/#conclusion","title":"Conclusion","text":"<p>Vous avez maintenant expos\u00e9 votre registry interne OpenShift. Vous pouvez pousser et r\u00e9cup\u00e9rer des images via la route expos\u00e9e.</p> <p>Auteur : Romain GASQUET</p>","tags":["OpenShift","Configuration","Registry"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/","title":"De l'Annotation \u00e0 la D\u00e9tection : Entra\u00eener un Mod\u00e8le YOLO sur OpenShift AI","text":"","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#introduction","title":"Introduction","text":"<p>Dans le domaine de la vision par ordinateur, la performance d'un mod\u00e8le de d\u00e9tection d'objets d\u00e9pend massivement de la qualit\u00e9 de son jeu de donn\u00e9es d'entra\u00eenement. Un bon mod\u00e8le n\u00e9cessite des milliers d'exemples correctement annot\u00e9s, une t\u00e2che qui peut rapidement devenir complexe et fastidieuse. C'est ici qu'interviennent des outils sp\u00e9cialis\u00e9s comme CVAT et des mod\u00e8les de pointe comme YOLO.</p> <p>CVAT (Computer Vision Annotation Tool) est une plateforme open-source puissante, con\u00e7ue pour simplifier et acc\u00e9l\u00e9rer l'annotation d'images et de vid\u00e9os. De son c\u00f4t\u00e9, YOLO (You Only Look Once) est l'un des mod\u00e8les de d\u00e9tection d'objets en temps r\u00e9el les plus populaires et performants.</p> <p>Cet article vous guidera pas \u00e0 pas \u00e0 travers le processus complet : de l'annotation d'un jeu de donn\u00e9es personnalis\u00e9 avec CVAT \u00e0 l'exportation des annotations au format YOLO, pour finalement entra\u00eener notre propre mod\u00e8le de d\u00e9tection d'objets.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#code-source","title":"Code Source","text":"<p>Le d\u00e9p\u00f4t GitHub ci-dessous contient tous les notebooks, scripts et fichiers de configuration n\u00e9cessaires pour suivre ce tutoriel et entra\u00eener votre propre mod\u00e8le YOLO.</p> <p>Acc\u00e9der au projet sur GitHub</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#1-recuperation-du-jeu-de-donnees-via-git","title":"1. R\u00e9cup\u00e9ration du Jeu de Donn\u00e9es via Git","text":"<p>Pour cette session d'annotation, le jeu de donn\u00e9es a \u00e9t\u00e9 pr\u00e9par\u00e9 et mis \u00e0 disposition dans un d\u00e9p\u00f4t Git. La premi\u00e8re \u00e9tape consiste donc \u00e0 le cloner sur votre machine locale.</p> <p>Ouvrez un terminal et ex\u00e9cutez la commande suivante :</p> <pre><code>git clone https://github.com/neutron-IT-organization/Data-Yolo-Test.git\n</code></pre> <p>Cette commande cr\u00e9era un nouveau dossier sur votre ordinateur, contenant l'ensemble des images pr\u00eates \u00e0 \u00eatre import\u00e9es dans CVAT.</p> <p>Le jeu de donn\u00e9es fourni a \u00e9t\u00e9 con\u00e7u pour \u00eatre repr\u00e9sentatif et diversifi\u00e9, afin de garantir la qualit\u00e9 des annotations et la future performance du mod\u00e8le. Il inclut des images sous diff\u00e9rents angles, \u00e9clairages, et avec des arri\u00e8re-plans vari\u00e9s. Une fois le clonage termin\u00e9, vous disposerez du dossier n\u00e9cessaire pour l'\u00e9tape suivante.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#2-deploiement-et-configuration-de-cvat","title":"2. D\u00e9ploiement et Configuration de CVAT","text":"","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#21-deploiement-de-cvat-sur-openshift","title":"2.1 D\u00e9ploiement de CVAT sur OpenShift","text":"<p>Avant de pouvoir annoter, nous devons d\u00e9ployer l'application CVAT sur notre cluster OpenShift.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#prerequis","title":"Pr\u00e9requis","text":"<p>Avant de commencer, assurez-vous de remplir les conditions suivantes : 1.  Installer <code>oc</code> : L'outil de ligne de commande d'OpenShift doit \u00eatre install\u00e9 sur votre machine. 2.  Avoir les droits admin : Vous devez disposer de permissions d'administrateur sur le cluster pour cr\u00e9er un projet et les ressources associ\u00e9es. 3.  \u00catre connect\u00e9 au cluster : Assurez-vous d'\u00eatre bien authentifi\u00e9 \u00e0 votre cluster via la commande <code>oc login</code>.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#etapes-du-deploiement","title":"\u00c9tapes du d\u00e9ploiement","text":"<p>Suivez ces \u00e9tapes depuis votre terminal pour d\u00e9ployer CVAT :</p> <ol> <li>Clonez le d\u00e9p\u00f4t Git contenant les fichiers de configuration de CVAT.</li> </ol> <pre><code>git clone git@github.com:neutron-IT-organization/CVAT.git\n</code></pre> <ol> <li>Cr\u00e9ez un nouveau projet (namespace) d\u00e9di\u00e9 \u00e0 CVAT.</li> </ol> <pre><code>oc new-project cvat\n</code></pre> <ol> <li>Naviguez dans le dossier des manifestes de d\u00e9ploiement.</li> </ol> <pre><code>cd CVAT/manifest\n</code></pre> <ol> <li>Appliquez les configurations.  Cette commande va cr\u00e9er toutes les ressources n\u00e9cessaires (d\u00e9ploiements, services, volumes, etc.) pour faire fonctionner l'application.</li> </ol> <pre><code>oc apply -f .\n</code></pre> <ol> <li> <p>Acc\u00e9dez \u00e0 l'interface web.  Une fois le d\u00e9ploiement termin\u00e9, allez dans la console web d'OpenShift, s\u00e9lectionnez le projet <code>cvat</code>, puis naviguez dans la section Networking &gt; Routes. Cliquez sur l'URL g\u00e9n\u00e9r\u00e9e pour ouvrir CVAT dans votre navigateur.</p> </li> <li> <p>R\u00e9cup\u00e9rez le mot de passe initial. Pour la premi\u00e8re connexion, un compte super-utilisateur a \u00e9t\u00e9 cr\u00e9\u00e9. Pour trouver son mot de passe :</p> <ul> <li>Dans le projet <code>cvat</code>, allez dans Secrets.</li> <li>Cherchez et cliquez sur le secret nomm\u00e9 <code>cvat-superuser</code>.</li> <li>Cliquez sur \"Reveal Values\" pour afficher le mot de passe. Le nom d'utilisateur est <code>admin</code>.</li> </ul> </li> </ol>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#22-creation-dune-organisation","title":"2.2 Cr\u00e9ation d'une Organisation","text":"<p>Pour travailler en \u00e9quipe, la meilleure pratique est de cr\u00e9er une \"Organisation\". Cela permet de partager les projets et de g\u00e9rer les permissions des diff\u00e9rents membres.</p> <ol> <li>Depuis le tableau de bord, cliquez sur votre nom d'utilisateur en haut \u00e0 droite, puis sur la fl\u00e8che <code>&gt;</code> \u00e0 c\u00f4t\u00e9 de Organization.</li> </ol> <p></p> <ol> <li> <p>Sur la page suivante, cliquez sur <code>+ Create</code> pour cr\u00e9er une nouvelle organisation.</p> </li> <li> <p>Remplissez le formulaire en donnant un nom court (Short name) \u00e0 votre organisation (ex: <code>orgTest</code>), puis cliquez sur <code>Submit</code>. </p> </li> <li> <p>Une fois cr\u00e9\u00e9e, vous basculez automatiquement dans le contexte de votre nouvelle organisation. </p> </li> <li> <p>Pour ajouter des collaborateurs, cliquez sur le bouton bleu <code>Invite members</code>. </p> </li> <li> <p>Dans la fen\u00eatre qui s'ouvre :</p> <ol> <li>Entrez l'adresse e-mail de la personne \u00e0 inviter.</li> <li>Choisissez son r\u00f4le (<code>Worker</code>, <code>Supervisor</code>, etc.).</li> <li>Cliquez sur <code>OK</code>. </li> </ol> </li> </ol>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#23-creation-du-projet-dannotation-sur-cvat","title":"2.3 Cr\u00e9ation du Projet d'Annotation sur CVAT","text":"<p>Une fois CVAT d\u00e9ploy\u00e9 et que vous \u00eates connect\u00e9, vous arriverez sur le tableau de bord. C'est ici que tous vos projets d'annotation seront list\u00e9s. La premi\u00e8re \u00e9tape consiste \u00e0 cr\u00e9er un nouveau projet pour nos images du Concorde.</p> <p>Comme indiqu\u00e9 sur la capture d'\u00e9cran, cliquez sur le bouton bleu <code>+</code> situ\u00e9 en haut \u00e0 droite de l'interface, puis s\u00e9lectionnez l'option <code>+ Create a new project</code>.</p> <p></p> <p>Vous serez alors redirig\u00e9 vers la page de configuration du projet. Vous devez y d\u00e9finir deux \u00e9l\u00e9ments essentiels :</p> <ol> <li>Name : Donnez un nom explicite \u00e0 votre projet, par exemple \"Aircraft Detection\".</li> <li>Labels : C'est ici que vous d\u00e9clarez les diff\u00e9rentes classes d'objets que vous allez annoter. Pour ce guide, nous nous concentrons sur une seule classe. Cliquez sur <code>Add label</code> et tapez le nom <code>Concorde</code>.</li> </ol> <p>Une fois le nom et le label configur\u00e9s comme sur l'image ci-dessous, cliquez sur le bouton <code>Submit</code> pour cr\u00e9er votre projet.</p> <p></p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#3-creation-dune-tache-dannotation","title":"3. Cr\u00e9ation d'une T\u00e2che d'Annotation","text":"<p>Dans CVAT, le travail d'annotation s'effectue au sein de t\u00e2ches (tasks). Une t\u00e2che contient un lot d'images \u00e0 annoter et les annotations qui lui sont associ\u00e9es. Un projet peut contenir plusieurs t\u00e2ches, ce qui est pratique pour diviser le travail ou organiser diff\u00e9rents lots de donn\u00e9es.</p> <p>Maintenant que notre projet est pr\u00eat, nous pouvons cr\u00e9er notre premi\u00e8re t\u00e2che. Depuis la page du projet, qui est pour l'instant vide, cliquez sur le bouton <code>+</code> situ\u00e9 dans le coin inf\u00e9rieur droit pour commencer.</p> <p> </p> <p>Cette action ouvre la page \"Create a new task\". Ici, vous configurez la t\u00e2che et y ajoutez vos donn\u00e9es.</p> <ol> <li>Name : Donnez un nom \u00e0 la t\u00e2che. C'est une bonne pratique de la nommer en fonction de son r\u00f4le dans le processus de Machine Learning, par exemple <code>Train</code>.</li> <li>Select files : C'est ici que vous importez les images pr\u00e9par\u00e9es \u00e0 la premi\u00e8re \u00e9tape. Assurez-vous d'\u00eatre sur l'onglet <code>My computer</code>, puis cliquez sur la zone \"Click or drag files to this area\". Vous pouvez alors s\u00e9lectionner plusieurs images ou directement le dossier complet qui les contient.</li> </ol> <p> </p> <p>Enfin, avant de soumettre, descendez jusqu'\u00e0 la section <code>Advanced configuration</code>. Assurez-vous que l'option <code>Choose format</code> est bien sur <code>CVAT for images 1.1</code>.</p> <p> </p> <p>Cliquez ensuite sur le bouton <code>Submit</code> en bas de la page pour finaliser la cr\u00e9ation de la t\u00e2che.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#4-lancement-de-lannotation","title":"4. Lancement de l'Annotation","text":"<p>Une fois la t\u00e2che cr\u00e9\u00e9e et les images import\u00e9es, nous sommes pr\u00eats \u00e0 passer au c\u0153ur du travail : l'annotation.</p> <p>Avant d'ouvrir l'\u00e9diteur, une bonne pratique consiste \u00e0 sp\u00e9cifier \u00e0 quel sous-ensemble de donn\u00e9es cette t\u00e2che appartient. Dans le champ <code>Subset</code>, s\u00e9lectionnez si ces images feront partie de votre jeu d'entra\u00eenement (<code>Train</code>), de test (<code>Test</code>), ou de validation (<code>Validation</code>). Cette m\u00e9tadonn\u00e9e sera utile lors de l'exportation.</p> <p>Ensuite, pour commencer \u00e0 dessiner les bo\u00eetes englobantes, descendez \u00e0 la section <code>Jobs</code>. Un \"job\" est l'unit\u00e9 de travail concr\u00e8te. Cliquez sur le lien du job (par exemple, <code>Job #...</code>) pour ouvrir l'interface d'annotation.</p> <p> </p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#linterface-dannotation","title":"L'Interface d'Annotation","text":"<p>Cette action ouvre l'\u00e9diteur principal. C'est ici que vous allez dessiner les bo\u00eetes englobantes sur chaque image. Le processus est simple et se d\u00e9roule en plusieurs \u00e9tapes, indiqu\u00e9es sur l'image ci-dessous :</p> <p> </p> <ol> <li>S\u00e9lectionner l'outil Rectangle : Dans la barre d'outils de gauche, cliquez sur l'ic\u00f4ne <code>Draw new rectangle</code> pour activer l'outil de dessin.</li> <li>Choisir le bon Label : Juste en dessous, assurez-vous que le label s\u00e9lectionn\u00e9 est bien celui que vous voulez annoter, ici <code>Concorde</code>.</li> <li>Confirmer la Forme : V\u00e9rifiez que la m\u00e9thode de dessin est bien \"Rectangle\".</li> <li>Dessiner la Bo\u00eete : Sur l'image, cliquez et faites glisser votre souris pour dessiner une bo\u00eete qui entoure l'objet d'int\u00e9r\u00eat le plus pr\u00e9cis\u00e9ment possible.</li> <li>Naviguer : Une fois l'objet annot\u00e9 sur une image, utilisez les fl\u00e8ches en haut de l'\u00e9cran pour passer \u00e0 l'image suivante (<code>&gt;</code>) et r\u00e9p\u00e9ter l'op\u00e9ration.</li> </ol> <p>R\u00e9p\u00e9tez ce processus pour toutes les images de votre t\u00e2che. Pensez \u00e0 cliquer r\u00e9guli\u00e8rement sur l'ic\u00f4ne de sauvegarde pour enregistrer votre travail. Une fois que vous avez termin\u00e9, vous pouvez simplement fermer l'onglet ou revenir \u00e0 la page de la t\u00e2che ; votre travail est sauvegard\u00e9 sur le serveur CVAT.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#5-exporter-les-annotations-au-format-yolo","title":"5. Exporter les Annotations au Format YOLO","text":"<p>Une fois que toutes les images ont \u00e9t\u00e9 soigneusement annot\u00e9es, il est temps d'exporter notre travail dans un format directement utilisable par YOLO.</p> <p>Pour cela, retournez sur la page de la t\u00e2che et suivez ces deux \u00e9tapes :</p> <ol> <li>Valider le travail : Dans la liste des \"Jobs\", il est bon de changer l'\u00e9tat (Stage) \u00e0 <code>acceptance</code>. Cela permet de marquer officiellement que le travail d'annotation pour ce lot est termin\u00e9 et valid\u00e9.</li> <li>Lancer l'exportation : En haut \u00e0 droite de la page, cliquez sur le bouton <code>Actions</code>. Dans le menu qui appara\u00eet, s\u00e9lectionnez <code>Export task dataset</code>.</li> </ol> <p> </p> <p>Une fen\u00eatre de dialogue s'ouvre alors. C'est ici que vous finalisez les options d'exportation :</p> <ul> <li>Export format : Choisissez <code>YOLO 1.1</code> dans la liste. C'est essentiel pour obtenir des fichiers d'annotation compatibles.</li> <li>Save images : Cochez cette case pour inclure les images avec leurs fichiers d'annotation dans l'archive <code>.zip</code> qui sera g\u00e9n\u00e9r\u00e9e.</li> </ul> <p>Une fois ces options configur\u00e9es, cliquez sur <code>OK</code> pour lancer lt\u00e9l\u00e9chargement du fichier.</p> <p></p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#6-preparation-et-centralisation-du-dataset-sur-minio","title":"6. Pr\u00e9paration et Centralisation du Dataset sur MinIO","text":"<p>Apr\u00e8s avoir export\u00e9 vos archives <code>.zip</code> depuis CVAT pour chaque lot (train, test, validation), l'\u00e9tape suivante consiste \u00e0 organiser ces fichiers dans une structure de dossiers propre. Cette organisation est cruciale pour que le script d'entra\u00eenement puisse retrouver facilement les donn\u00e9es.</p> <p>Sur votre machine locale, cr\u00e9ez un dossier principal qui contiendra l'ensemble de votre dataset (par exemple, <code>data_CVAT</code>). \u00c0 l'int\u00e9rieur de celui-ci, d\u00e9compressez vos archives pour obtenir les sous-dossiers <code>test</code>, <code>train</code>, et <code>valid</code>. Chacun de ces dossiers doit contenir \u00e0 la fois les images et leurs fichiers d'annotation <code>.txt</code> correspondants.</p> <p>Votre structure de fichiers finale devrait ressembler \u00e0 cet exemple :</p> <p></p> <p>Une fois que votre dataset est bien organis\u00e9, la derni\u00e8re \u00e9tape est de t\u00e9l\u00e9verser le dossier principal (<code>data_CVAT</code>) sur votre bucket MinIO. Cela permet de centraliser vos donn\u00e9es et de les rendre accessibles pour votre environnement d'entra\u00eenement, comme un notebook sur OpenShift AI.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#7-importation-du-dataset-pour-lentrainement","title":"7. Importation du Dataset pour l'Entra\u00eenement","text":"<p>Maintenant que notre dataset est proprement organis\u00e9 et centralis\u00e9 sur MinIO, nous devons \u00e9tablir la connexion entre notre notebook et le serveur MinIO. Pour des raisons de s\u00e9curit\u00e9, les identifiants (cl\u00e9s d'acc\u00e8s) ne sont jamais \u00e9crits directement dans le code. Sur OpenShift AI, la m\u00e9thode recommand\u00e9e est d'utiliser une \"Connexion de donn\u00e9es\", qui stocke ces informations de mani\u00e8re s\u00e9curis\u00e9e et les rend disponibles pour votre workbench.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#71-configuration-via-linterface-dopenshift-ai","title":"7.1. Configuration via l'Interface d'OpenShift AI","text":"<p>La connexion se fait en deux temps, directement dans l'interface graphique :</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#711-creer-une-connexion-de-donnees","title":"7.1.1. Cr\u00e9er une Connexion de Donn\u00e9es :","text":"<p>Dans votre projet sur OpenShift AI, allez dans la section \"Connexions de donn\u00e9es\" et cliquez sur \"Ajouter une connexion de donn\u00e9es\". Remplissez les champs avec les informations de votre serveur MinIO :</p> <p>Nom de la connexion : Un nom descriptif (ex: Connexion MinIO Projet Yolo).</p> <p>Access key ID : Votre cl\u00e9 d'acc\u00e8s MinIO.</p> <p>Secret access key : Votre cl\u00e9 secr\u00e8te MinIO.</p> <p>Endpoint : L'URL de votre service MinIO.</p> <p> </p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#712-lier-la-connexion-au-workbench","title":"7.1.2. Lier la Connexion au Workbench :","text":"<p>Lors de la configuration de votre workbench (ou en le modifiant), descendez jusqu'\u00e0 la section \"Connexions de donn\u00e9es\". S\u00e9lectionnez dans la liste d\u00e9roulante la connexion que vous venez de cr\u00e9er.</p> <p> </p> <p>En faisant cela, OpenShift AI va automatiquement injecter les identifiants de cette connexion comme variables d'environnement (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_S3_ENDPOINT) dans l'environnement de votre notebook.</p> <p>Maintenant, nous allons utiliser un script Python dans notre environnement d'entra\u00eenement pour simplement t\u00e9l\u00e9charger ce dataset depuis MinIO et le rendre disponible localement pour l'entra\u00eenement.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#72-configuration-et-connexion","title":"7.2. Configuration et Connexion","text":"<p>La premi\u00e8re partie du script consiste \u00e0 d\u00e9finir les chemins et \u00e0 \u00e9tablir une connexion s\u00e9curis\u00e9e avec notre serveur MinIO en utilisant les variables d'environnement.</p> <pre><code>import os\nimport boto3\nimport shutil\nimport yaml\n\n# Configuration des chemins et du client MinIO\nOUTPUT_DATASET_PATH = \"concorde_yolo_dataset\"\nS3_SOURCE_PREFIX = \"data_CVAT\" # Le dossier t\u00e9l\u00e9vers\u00e9 \u00e0 l'\u00e9tape pr\u00e9c\u00e9dente\n\n# Connexion s\u00e9curis\u00e9e au client S3\nclient = boto3.client(\n    \"s3\",\n    endpoint_url=os.environ.get(\"AWS_S3_ENDPOINT\"),\n    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n)\n</code></pre>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#73-telechargement-et-restructuration","title":"7.3. T\u00e9l\u00e9chargement et Restructuration","text":"<p>La fonction principale du script parcourt les dossiers <code>train</code>, <code>test</code> et <code>valid</code> sur MinIO. Pour chaque fichier, elle le t\u00e9l\u00e9charge et le place dans la structure de dossiers requise par YOLO (<code>images/</code> et <code>labels/</code>).</p> <pre><code>def download_and_restructure_s3_folder(bucket, s3_prefix, local_base_path):\n    \"\"\"\n    T\u00e9l\u00e9charge et restructure un dossier depuis S3 (MinIO) vers la structure YOLO.\n    \"\"\"\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        paginator = client.get_paginator('list_objects_v2')\n        pages = paginator.paginate(Bucket=bucket, Prefix=s3_prefix)\n\n        all_objects = [obj for page in pages for obj in page.get('Contents', [])]\n\n        if not all_objects:\n            print(f\"Aucun objet trouv\u00e9 dans {bucket}/{s3_prefix}. Le script va s'arr\u00eater.\")\n            return\n\n        with tqdm(total=len(all_objects), desc=\"Traitement des fichiers\", unit=\"file\") as pbar:\n            futures = []\n            for obj in all_objects:\n                key = obj['Key']\n                if key.endswith('/'):\n                    pbar.update(1)\n                    continue\n\n                file_extension = os.path.splitext(key)[1].lower()\n                is_image = file_extension in ['.jpg', '.jpeg', '.png']\n                is_label = file_extension == '.txt'\n\n                if not is_image and not is_label:\n                    pbar.update(1)\n                    continue\n\n                try:\n                    split_type = key.replace(s3_prefix, '').strip('/').split('/')[0]\n                    if split_type == 'valid':\n                        split_type = 'valid'\n                except IndexError:\n                    continue\n\n                subfolder = 'images' if is_image else 'labels'\n                file_name = os.path.basename(key)\n                local_file_path = os.path.join(local_base_path, split_type, subfolder, file_name)\n\n                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n                future = executor.submit(client.download_file, bucket, key, local_file_path)\n                future.add_done_callback(lambda p: pbar.update(1))\n                futures.append(future)\n\n            for future in futures:\n                future.result()\n</code></pre>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#74-creation-du-fichier-datayaml","title":"7.4. Cr\u00e9ation du Fichier data.yaml","text":"<p>Une fois les donn\u00e9es structur\u00e9es, une fonction g\u00e9n\u00e8re le fichier <code>data.yaml</code>. Ce fichier est indispensable pour YOLO : il lui indique o\u00f9 trouver les diff\u00e9rents ensembles de donn\u00e9es et quelles sont les classes \u00e0 apprendre.</p> <pre><code>def create_yaml_file(output_path):\n    yaml_content = {\n        'train': os.path.abspath(os.path.join(output_path, 'train', 'images')),\n        'val': os.path.abspath(os.path.join(output_path, 'valid', 'images')),\n        'test': os.path.abspath(os.path.join(output_path, 'test', 'images')),\n        'nc': 1,\n        'names': ['Concorde']\n    }\n\n    yaml_file_path = os.path.join(output_path, 'data.yaml')\n    with open(yaml_file_path, 'w') as f:\n        yaml.dump(yaml_content, f, sort_keys=False)\n</code></pre>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#75-execution","title":"7.5. Ex\u00e9cution","text":"<p>Enfin, le script principal orchestre ces \u00e9tapes : il nettoie l'environnement, lance le t\u00e9l\u00e9chargement et la restructuration, puis cr\u00e9e le fichier <code>data.yaml</code>, rendant le dataset pr\u00eat pour l'entra\u00eenement.</p> <pre><code># --- 5. Script principal ---\nif __name__ == \"__main__\":\n    if os.path.exists(OUTPUT_DATASET_PATH):\n        print(f\"Nettoyage du r\u00e9pertoire existant : {OUTPUT_DATASET_PATH}\")\n        shutil.rmtree(OUTPUT_DATASET_PATH)\n\n    print(f\"D\u00e9but du t\u00e9l\u00e9chargement et de la restructuration depuis '{S3_SOURCE_PREFIX}'...\")\n    download_and_restructure_s3_folder(AWS_S3_BUCKET, S3_SOURCE_PREFIX, OUTPUT_DATASET_PATH)\n\n    create_yaml_file(OUTPUT_DATASET_PATH)\n\n    print(\"\\n Processus termin\u00e9. Votre dataset est pr\u00eat dans le dossier :\", OUTPUT_DATASET_PATH)\n</code></pre>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#8-entrainement-du-modele-avec-yolo","title":"8. Entra\u00eenement du Mod\u00e8le avec YOLO","text":"<p>Maintenant que notre dataset est import\u00e9, structur\u00e9 et pr\u00eat \u00e0 l'emploi avec son fichier <code>data.yaml</code>, nous pouvons passer \u00e0 l'\u00e9tape la plus excitante : l'entra\u00eenement de notre mod\u00e8le de d\u00e9tection d'objets. Le processus utilisera le transfer learning, une technique qui consiste \u00e0 partir d'un mod\u00e8le d\u00e9j\u00e0 pr\u00e9-entra\u00een\u00e9 pour l'adapter \u00e0 notre besoin sp\u00e9cifique, ce qui est plus rapide et efficace.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#81-preparation-du-modele-de-base-yolo","title":"8.1. Pr\u00e9paration du Mod\u00e8le de Base YOLO","text":"<p>Pour le transfer learning, notre point de d\u00e9part est un mod\u00e8le pr\u00e9-entra\u00een\u00e9, g\u00e9n\u00e9reusement mis \u00e0 disposition par l'\u00e9quipe Ultralytics. Nous allons le t\u00e9l\u00e9charger directement depuis leur d\u00e9p\u00f4t GitHub.</p> <p>Acc\u00e9der au D\u00e9p\u00f4t GitHub</p> <p>Rendez-vous sur la page officielle du projet en cliquant sur ce lien : https://github.com/ultralytics/ultralytics/tree/main?tab=readme-ov-file.</p> <p>Trouver le Tableau des Mod\u00e8les</p> <p>Sur la page principale, faites d\u00e9filer vers le bas jusqu'\u00e0 la section du <code>README</code> qui pr\u00e9sente les diff\u00e9rents mod\u00e8les disponibles. Vous y trouverez un tableau intitul\u00e9 \"Detection (COCO)\", comme celui ci-dessous. Ce tableau liste les diff\u00e9rentes variantes du mod\u00e8le, leurs performances et leurs tailles.</p> <p> </p> <p>T\u00e9l\u00e9charger le Mod\u00e8le</p> <p>Chaque nom dans la colonne \"Model\" est un lien de t\u00e9l\u00e9chargement direct. Pour notre guide, nous allons utiliser la version \"nano\", qui est l\u00e9g\u00e8re et rapide, id\u00e9ale pour commencer.</p> <p>Rep\u00e9rez la ligne <code>YOLOv11n</code> et cliquez dessus </p> <p>Le t\u00e9l\u00e9chargement du fichier <code>yolov11n.pt</code> d\u00e9marrera automatiquement.</p> <p>Une fois que vous avez ce fichier sur votre ordinateur, l'\u00e9tape suivante consiste \u00e0 le t\u00e9l\u00e9verser dans votre bucket MinIO, o\u00f9 il sera accessible pour notre script d'entra\u00eenement.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#82-installation-et-configuration","title":"8.2. Installation et Configuration","text":"<p>La premi\u00e8re \u00e9tape dans notre notebook d'entra\u00eenement est d'installer la librairie <code>ultralytics</code> qui contient l'impl\u00e9mentation de YOLO, puis de d\u00e9finir nos configurations.</p> <pre><code># 1. Installation de la librairie Ultralytics\n!pip install ultralytics\n\n# 2. Import des librairies n\u00e9cessaires\nfrom ultralytics import YOLO\nimport os\n\n# 3. Configuration des hyperparam\u00e8tres de l'entra\u00eenement\nEPOCHS = 100\nIMGSZ = 640\nNB_FROZEN_LAYER = 10 # Nombre de couches du mod\u00e8le \u00e0 \"geler\" (ne pas r\u00e9-entra\u00eener)\n\n# 4. D\u00e9finition des chemins importants\nDATA_CONFIG_PATH = './concorde_yolo_dataset/data.yaml'\nBASE_MODEL_PATH = \"yolov11n.pt\" # Mod\u00e8le pr\u00e9-entra\u00een\u00e9 qui servira de base\n</code></pre> <p>Ici, nous d\u00e9finissons des param\u00e8tres cl\u00e9s comme le nombre d'\u00e9poques (cycles d'entra\u00eenement), la taille des images, et le nombre de couches du mod\u00e8le que nous allons \"geler\". Geler des couches signifie qu'elles ne seront pas modifi\u00e9es pendant l'entra\u00eenement, conservant ainsi les connaissances g\u00e9n\u00e9rales acquises lors de leur pr\u00e9-entra\u00eenement.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#83-chargement-du-modele-de-base","title":"8.3. Chargement du Mod\u00e8le de Base","text":"<p>Nous n'entra\u00eenons pas un mod\u00e8le \u00e0 partir de z\u00e9ro. Nous t\u00e9l\u00e9chargeons un mod\u00e8le YOLOv11 pr\u00e9-entra\u00een\u00e9 (<code>yolov11n.pt</code>) depuis notre stockage MinIO. Ce mod\u00e8le sait d\u00e9j\u00e0 reconna\u00eetre des formes et des textures g\u00e9n\u00e9rales, nous n'avons plus qu'\u00e0 lui apprendre \u00e0 reconna\u00eetre sp\u00e9cifiquement le \"Concorde\".</p> <pre><code># Connexion au client S3...\n# ...\n# T\u00e9l\u00e9chargement du mod\u00e8le de base depuis MinIO\nclient.download_file(AWS_S3_BUCKET, BASE_MODEL_PATH, \"yolov11n.pt\")\n</code></pre>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#84-lancement-de-lentrainement","title":"8.4. Lancement de l'Entra\u00eenement","text":"<p>Avec le mod\u00e8le de base et le dataset pr\u00eats, une seule ligne de code suffit pour lancer l'entra\u00eenement.</p> <pre><code># Charger le mod\u00e8le pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov11n.pt')\n\n# Lancer l'entra\u00eenement\nresults = model.train(\n    data=DATA_CONFIG_PATH, \n    epochs=EPOCHS, \n    imgsz=IMGSZ, \n    freeze=NB_FROZEN_LAYER,\n    batch=-1 # -1 pour un ajustement automatique du batch size\n)\n</code></pre> <p>YOLO s'occupe de tout : il charge les donn\u00e9es, augmente le dataset (data augmentation), entra\u00eene le mod\u00e8le pendant 100 \u00e9poques, et sauvegarde les r\u00e9sultats, y compris les \"poids\" du meilleur mod\u00e8le obtenu.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#85-exportation-et-sauvegarde-des-resultats","title":"8.5. Exportation et Sauvegarde des R\u00e9sultats","text":"<p>Une fois l'entra\u00eenement termin\u00e9, la bonne pratique est d'exporter le mod\u00e8le au format ONNX. C'est un format standard qui facilite le d\u00e9ploiement sur diff\u00e9rentes plateformes.</p> <p>Ensuite, nous t\u00e9l\u00e9versons les r\u00e9sultats importants sur MinIO pour les conserver et les utiliser plus tard :</p> <ul> <li>Le meilleur mod\u00e8le au format <code>.pt</code> (natif PyTorch).</li> <li>Le mod\u00e8le export\u00e9 au format <code>.onnx</code>.</li> <li>Le fichier <code>results.csv</code> qui contient les m\u00e9triques de performance de l'entra\u00eenement.</li> </ul> <pre><code># Charger les meilleurs poids obtenus apr\u00e8s l'entra\u00eenement\nmodel = YOLO('runs/detect/train/weights/best.pt')\n\n# Exporter au format ONNX\nmodel.export(format=\"onnx\")\n\n# T\u00e9l\u00e9verser les mod\u00e8les et les r\u00e9sultats sur MinIO\nclient.upload_file('runs/detect/train/weights/best.onnx', ...)\nclient.upload_file('runs/detect/train/weights/best.pt', ...)\nclient.upload_file('runs/detect/train/results.csv', ...)\n</code></pre>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#9-comparaison-et-validation-des-resultats","title":"9. Comparaison et Validation des R\u00e9sultats","text":"<p>Apr\u00e8s avoir entra\u00een\u00e9 notre mod\u00e8le, l'\u00e9tape finale est de v\u00e9rifier qu'il est bien meilleur que le mod\u00e8le de base pour notre t\u00e2che sp\u00e9cifique. Nous allons effectuer une comparaison \u00e0 la fois quantitative (avec des m\u00e9triques de performance) et qualitative (en visualisant les d\u00e9tections).</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#91-comparaison-quantitative-mesurer-la-performance","title":"9.1. Comparaison Quantitative : Mesurer la Performance","text":"<p>La premi\u00e8re m\u00e9thode est de mesurer la performance de chaque mod\u00e8le sur notre jeu de test \u00e0 l'aide de la fonction <code>.val()</code>. Cette fonction calcule des m\u00e9triques standards comme la mAP (mean Average Precision), qui est un excellent indicateur de la pr\u00e9cision d'un mod\u00e8le de d\u00e9tection.</p> <p>Le script charge les deux mod\u00e8les (le mod\u00e8le de base et notre nouveau mod\u00e8le) puis ex\u00e9cute la validation sur les deux.</p> <pre><code>from ultralytics import YOLO\n\n# Charger les deux mod\u00e8les\nmodel_base = YOLO(\"/tmp/base-model.pt\") # Le mod\u00e8le YOLOv11n original\nmodel_new = YOLO(\"/tmp/new-model.pt\")  # Notre mod\u00e8le fine-tun\u00e9\n\n# Lancer la validation pour chaque mod\u00e8le sur le m\u00eame jeu de test\nprint(\"Validation du mod\u00e8le de base :\")\nresults_base = model_base.val(data=\"concorde_yolo_dataset/data.yaml\")\n\nprint(\"\\\\nValidation de notre nouveau mod\u00e8le :\")\nresults_new = model_new.val(data=\"concorde_yolo_dataset/data.yaml\")\n</code></pre> <p>L'objectif est de constater que la mAP pour la classe \"Concorde\" est tr\u00e8s faible (voire nulle) pour le mod\u00e8le de base, et significativement plus \u00e9lev\u00e9e pour notre mod\u00e8le sp\u00e9cialis\u00e9.</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#92-comparaison-qualitative-visualiser-la-detection","title":"9.2. Comparaison Qualitative : Visualiser la D\u00e9tection","text":"<p>Les chiffres sont importants, mais une inspection visuelle est souvent plus parlante. Le script suivant s\u00e9lectionne al\u00e9atoirement 5 images de notre jeu de test et lance la d\u00e9tection avec les deux mod\u00e8les pour chacune d'entre elles.</p> <pre><code>import random\n\n# S\u00e9lectionner 5 images de test au hasard\nimage_filenames = os.listdir(\"concorde_yolo_dataset/test/images\")\nrandom_images = random.sample(image_filenames, 5)\n\n# Pour chaque image, afficher la d\u00e9tection des deux mod\u00e8les\nfor image_file in random_images:\n    test_image_path = os.path.join(\"concorde_yolo_dataset/test/images\", image_file)\n\n    # Inf\u00e9rence avec le mod\u00e8le de base\n    print(f\"\\\\n--- D\u00e9tection avec le mod\u00e8le DE BASE sur {image_file} ---\")\n    res_base = model_base(test_image_path)\n    res_base[0].show()\n\n    # Inf\u00e9rence avec notre nouveau mod\u00e8le\n    print(f\"--- D\u00e9tection avec notre NOUVEAU mod\u00e8le sur {image_file} ---\")\n    res_new = model_new(test_image_path)\n    res_new[0].show()\n</code></pre> <p>Le r\u00e9sultat attendu est clair : le mod\u00e8le de base d\u00e9tectera l'avion mais avec une \u00e9tiquette g\u00e9n\u00e9rique (\"airplane\"), tandis que notre mod\u00e8le dessinera une bo\u00eete pr\u00e9cise avec le label correct \"Concorde\", prouvant ainsi le succ\u00e8s de notre sp\u00e9cialisation.</p> <p> </p> <p>Il est \u00e9galement important de noter le comportement du mod\u00e8le face \u00e0 des avions qui ne sont pas des Concordes. Comme notre entra\u00eenement s'est focalis\u00e9 exclusivement sur la classe \"Concorde\", le mod\u00e8le a appris \u00e0 ignorer les autres types d'avion, comme le montre l'exemple ci-dessous. C'est la preuve de sa sp\u00e9cialisation : il ne se contente pas de trouver des avions, il trouve uniquement des Concordes.</p> <p> </p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/CVAT-YOLO/#conclusion-generale","title":"Conclusion G\u00e9n\u00e9rale","text":"<p>\u00c0 travers ce guide, nous avons parcouru l'ensemble du cycle de vie d'un projet de vision par ordinateur : de la collecte d'images \u00e0 l'annotation pr\u00e9cise avec CVAT, en passant par la structuration des donn\u00e9es et l'entra\u00eenement d'un mod\u00e8le YOLO sur OpenShift. Vous avez pu constater comment le transfer learning permet, avec un jeu de donn\u00e9es relativement modeste, de sp\u00e9cialiser un mod\u00e8le puissant pour une t\u00e2che de d\u00e9tection personnalis\u00e9e.</p> <p>La cl\u00e9 du succ\u00e8s r\u00e9side dans la qualit\u00e9 des donn\u00e9es annot\u00e9es et une m\u00e9thodologie structur\u00e9e. Vous \u00eates maintenant pr\u00eat \u00e0 appliquer ces \u00e9tapes \u00e0 vos propres projets pour d\u00e9tecter n'importe quel objet !</p> <p>Auteur : Mohamed-Reda BOUAMOUD</p>","tags":["OpenShift","IA","YOLO","CVAT","Object Detection","Computer Vision","Machine Learning"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/","title":"D\u00e9ploiement de DeepSeek sur OpenShift","text":"","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#prerequis","title":"Pr\u00e9requis","text":"<p>Avant de commencer, assurez-vous de remplir les conditions suivantes :</p> <ul> <li>\u00catre en mesure de se connecter \u00e0 un cluster OpenShift.</li> <li>Pouvoir cr\u00e9er un namespace sur OpenShift.</li> <li>Avoir les privil\u00e8ges administrateur sur ce namespace.</li> <li>Cloner le d\u00e9p\u00f4t suivant pour disposer des fichiers n\u00e9cessaires :</li> </ul> <pre><code>git clone https://github.com/neutron-IT-organization/deepseek-openshift.git\n</code></pre>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#1-creer-un-namespace-deepseek","title":"1. Cr\u00e9er un Namespace deepseek","text":"<p>Pour commencer, cr\u00e9ez un namespace d\u00e9di\u00e9 \u00e0 DeepSeek sur OpenShift :</p> <pre><code>oc create namespace deepseek\n</code></pre>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#2-configurez-les-scc","title":"2. Configurez les SCC","text":"<p>Attribuez la SCC (Security Context Constraint) <code>privileged</code> au service account ci-dessous dans le namespace <code>deepseek</code> :</p> <pre><code>oc adm policy add-scc-to-user privileged -z default -n deepseek\noc adm policy add-scc-to-user privileged -z dify-redis -n deepseek\noc adm policy add-scc-to-user privileged -z dify-postgres -n deepseek\noc adm policy add-scc-to-user privileged -z dify-weaviate -n deepseek\n</code></pre>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#3-installer-le-deploiement-dollama","title":"3. Installer le d\u00e9ploiement d'Ollama","text":"<p>D\u00e9ployez le manifest manifest/ollama.yaml dans le namespace <code>deepseek</code> avec le fichier de configuration suivant :</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ollama\n  namespace: deepseek\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ollama\n  template:\n    metadata:\n      labels:\n        app: ollama\n    spec:\n      securityContext:\n        runAsUser: 0\n      containers:\n        - name: ollama\n          image: ollama/ollama:latest\n          ports:\n            - containerPort: 11434\n              protocol: TCP\n          resources: {}\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n</code></pre> <p>Appliquez cette configuration avec la commande suivante :</p> <pre><code>oc apply -f manifest/ollama.yaml\n</code></pre>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#4-executer-deepseek-v2-dans-ollama","title":"4. Ex\u00e9cuter deepseek-v2 dans ollama","text":"<p>Acc\u00e9dez \u00e0 l'onglet Workloads &gt; Pods.</p> <p>Recherchez le pod ollama dans le namespace deepseek.</p> <p>Cliquez dessus, puis allez dans l'onglet Terminal.</p> <p>Ex\u00e9cutez la commande suivante directement dans le terminal du pod :</p> <p>Une fois le d\u00e9ploiement d'Ollama effectu\u00e9, ex\u00e9cutez la commande suivante \u00e0 l'int\u00e9rieur du pod pour lancer <code>deepseek-v2</code> :</p> <pre><code>ollama run deepseek-v2\n</code></pre> <p></p>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#alternative-en-ligne-de-commande","title":"Alternative en ligne de commande","text":"<pre><code>oc exec -it &lt;pod-name&gt; -- ollama run deepseek-v2\n</code></pre> <p>Remplacez <code>&lt;pod-name&gt;</code> par le nom du pod Ollama que vous pouvez obtenir avec la commande <code>oc get pods</code>.</p>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#5-installer-dify-dans-le-namespace-deepseek","title":"5. Installer Dify dans le namespace <code>deepseek</code>","text":"<p>Ajoutez le repo Helm de Dify et installez-le dans le namespace <code>deepseek</code> :</p> <pre><code>oc apply -f manifest/dify.yaml\n</code></pre> <p></p>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#6-exposer-dify-avec-une-route","title":"6. Exposer Dify avec une Route","text":"<p>Enfin, exposez le service Dify via une route OpenShift pour rendre l'application accessible :</p> <pre><code>kind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: dify-route\n  namespace: deepseek\nspec:\n  path: /\n  to:\n    kind: Service\n    name: dify-nginx\n    weight: 100\n  port:\n    targetPort: dify-nginx\n  tls:\n    termination: edge\n  wildcardPolicy: None\n</code></pre> <p>Appliquez cette route avec la commande suivante :</p> <pre><code>oc apply -f manifest/dify-route.yaml\n</code></pre> <p>Vous pouvez maintenant acc\u00e9der a dify via la route. Pour la r\u00e9cup\u00e9rer utilisez la commande ci dessous </p> <pre><code>oc get route dify-route -n deepseek -o jsonpath='{.spec.host}'\n</code></pre>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#7-configuration-de-dify","title":"7. Configuration de dify","text":"<p>Lors de la premiere connextion a dify, le mot de passe administrateur vous sera demand\u00e9. Par d\u00e9faut celui-ci prend la valeur <code>password</code>.</p> <p></p> <p>Compl\u00e9tez ensuite le formulaire <code>Setting up an admin account</code> avec un user/password.</p> <p></p> <p>Vous devriez maintenant acc\u00e9der a l'interface Dify.</p> <p></p>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#8-create-deepseek-chatbot","title":"8. Create Deepseek chatbot","text":"<p>Dans un premier temps configurez le model deepseek. Pour cela cliquez sur votre nom en haut a droite puis sur Settings. Puis cliquez sur Model Provider.</p> <p></p> <p>Selectionnez ensuite Ollama et cliquez sur <code>+ Add Model</code></p> <p>Compl\u00e9tez le formulaire.</p> <p>Dans Model Name, indiquez le nom de votre model. Ici <code>deepseek-v2</code>. Dans Base URL ecrivez le nom du service ollama <code>http://ollama.deepseek.svc.cluster.local</code>.</p> <p></p> <p>NOTE: Rechargez ensuite votre page pour que le model soit bien pris en compte. Pour cela vous pouvez cliquez sur F5.</p> <p>Vous pouvez maintenant cr\u00e9er votre chatbot. Pour cela dans l'onglet Studio, cliquez sur Chatbot puis sur <code>+ Create from blank</code>.</p> <p>Donnez un nom a votre chatbot puis cliquez sur create.</p> <p></p> <p>Votre chatbot est des a present fonctionnel vous pouvez le tester en discantant dans la section <code>Talk to bot</code>.</p> <p></p>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#9-publier-deepseek-chatbot","title":"9. Publier Deepseek chatbot","text":"<p>Vous pouvez maintenant publier votre chatbot.</p> <p>Pour cela cliquez sur Publish. Penser a <code>update</code> votre chatbot. Cliquez sur Run App. </p> <p>NOTE: Selon la configuration de votre nginx, il est possible que l'url utilis\u00e9 soit dify-api:5001. Si c'est le cas remplacer dans l'url dify-api:5001 par votre route pour acc\u00e9der au chatbot.</p> <p></p>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/DeepSeek/#conclusion","title":"Conclusion","text":"<p>Vous avez maintenant d\u00e9ploy\u00e9 DeepSeek sur OpenShift. Vous pouvez acc\u00e9der \u00e0 l'application via la route expos\u00e9e et utiliser les diff\u00e9rents services qu'elle offre.</p> <p>Auteur : Florian EVEN</p>","tags":["OpenShift","IA","Ollama","DeepSeek"]},{"location":"OpenShift/Intelligence-Artificielle/EdgeIA/","title":"L'Edge IA dans l'\u00e9cosyst\u00e8me RedHat","text":"","tags":["OpenShift","IA","Edge","OpenShift AI","BootC","MicroShift"]},{"location":"OpenShift/Intelligence-Artificielle/EdgeIA/#1-quest-ce-que-ledge-ia","title":"1. Qu'est-ce que l'Edge IA","text":"<p>L'Edge IA, ou intelligence artificielle en p\u00e9riph\u00e9rie, d\u00e9signe le traitement des donn\u00e9es par des algorithmes d'IA directement sur les appareils o\u00f9 les donn\u00e9es sont g\u00e9n\u00e9r\u00e9es, plut\u00f4t que dans un data center centralis\u00e9. Cela permet de r\u00e9duire la latence, d'am\u00e9liorer la s\u00e9curit\u00e9 et de diminuer la bande passante n\u00e9cessaire pour le transfert de donn\u00e9es.</p>","tags":["OpenShift","IA","Edge","OpenShift AI","BootC","MicroShift"]},{"location":"OpenShift/Intelligence-Artificielle/EdgeIA/#11-avantages-de-ledge-ia","title":"1.1 Avantages de l'Edge IA","text":"<ul> <li>R\u00e9duction de la latence : En traitant les donn\u00e9es localement, le temps de r\u00e9ponse peut \u00eatre rapide et ma\u00eetris\u00e9 afin de r\u00e9pondre aux enjeux industriels.</li> <li>S\u00e9curit\u00e9 am\u00e9lior\u00e9e : Les donn\u00e9es sensibles peuvent \u00eatre trait\u00e9es localement, r\u00e9duisant les risques de fuites.</li> <li>Bande passante optimis\u00e9e : Moins de donn\u00e9es doivent \u00eatre envoy\u00e9es vers le cloud, ce qui r\u00e9duit les co\u00fbts et la congestion du r\u00e9seau.</li> <li>Autonomie : Les appareils peuvent fonctionner de mani\u00e8re autonome, m\u00eame sans connexion Internet.</li> <li>\u00c9volutivit\u00e9 : Facilite l'ajout de nouveaux appareils et capteurs sans surcharger le r\u00e9seau central.</li> </ul>","tags":["OpenShift","IA","Edge","OpenShift AI","BootC","MicroShift"]},{"location":"OpenShift/Intelligence-Artificielle/EdgeIA/#12-cas-dutilisation","title":"1.2 Cas d'utilisation","text":"<ul> <li>V\u00e9hicules autonomes : Traitement en temps r\u00e9el des donn\u00e9es des capteurs pour une conduite s\u00e9curis\u00e9e.</li> <li>Usines intelligentes : Surveillance et maintenance pr\u00e9dictive des \u00e9quipements.</li> <li>Robotique : Gestion des mouvements dans l'espace et reconnaissance d'objets.</li> <li>Sant\u00e9 : Surveillance en temps r\u00e9el des patients et diagnostic rapide.</li> <li>Villes intelligentes : Gestion du trafic, surveillance environnementale et s\u00e9curit\u00e9 publique.</li> </ul>","tags":["OpenShift","IA","Edge","OpenShift AI","BootC","MicroShift"]},{"location":"OpenShift/Intelligence-Artificielle/EdgeIA/#2-entrainer-fine-tuner-deployer-son-modele-avec-openshift-ai","title":"2. Entra\u00eener, Fine-Tuner, D\u00e9ployer son mod\u00e8le avec OpenShift AI","text":"<p>La premi\u00e8re \u00e9tape de votre projet Edge IA est la cr\u00e9ation de votre mod\u00e8le pour r\u00e9pondre \u00e0 vos besoins sp\u00e9cifiques. Pour cette \u00e9tape, OpenShift AI est un alli\u00e9 de choix vous permettant dans un m\u00eame environnement au sein d'un cluster OpenShift de :</p> <ul> <li>Mettre en place un workbench pour cr\u00e9er votre environnement isol\u00e9 Jupyter Notebook.</li> <li>Mettre en place des pipelines pour automatiser les processus de formation et de d\u00e9ploiement.</li> <li>D\u00e9ployer des mod\u00e8les pour les tester et les valider.</li> </ul> <p>OpenShift AI vous permet de g\u00e9rer l'int\u00e9gralit\u00e9 du cycle de vie de votre mod\u00e8le, de la cr\u00e9ation \u00e0 la mise en production.</p>","tags":["OpenShift","IA","Edge","OpenShift AI","BootC","MicroShift"]},{"location":"OpenShift/Intelligence-Artificielle/EdgeIA/#3-compiler-tester-deployer-ses-micro-services-avec-openshift-et-tekton","title":"3. Compiler, Tester, D\u00e9ployer ses micro-services avec OpenShift et Tekton","text":"<p>Une fois votre mod\u00e8le s\u00e9lectionn\u00e9, entra\u00een\u00e9 et test\u00e9, vient le d\u00e9veloppement de vos micro-services pour capter et traiter vos donn\u00e9es ainsi que pour ex\u00e9cuter la r\u00e9ponse adapt\u00e9e. Par exemple, vous pourriez vouloir manipuler un objet dans un environnement 3D \u00e0 l'aide d'un bras robot et de cam\u00e9ras 3D. Dans ce cadre-l\u00e0, vous souhaiteriez d\u00e9velopper, tester et d\u00e9ployer les micro-services suivants :</p> <ul> <li>Acquisition des images et des nuages de points.</li> <li>Mod\u00e8le IA de reconnaissance d'objets.</li> <li>Calcul des coordonn\u00e9es des objets d\u00e9tect\u00e9s.</li> <li>Commande du bras robot.</li> <li>MQTT et/ou Kafka.</li> </ul> <p>Dans ce cadre-ci, OpenShift et Tekton vont vous permettre de r\u00e9aliser votre cha\u00eene CI/CD de d\u00e9veloppement pour v\u00e9rifier la qualit\u00e9 de votre code, construire et pousser vos images conteneurs dans votre registry, et d\u00e9ployer vos micro-services dans un environnement de test au sein de votre cluster OpenShift.</p>","tags":["OpenShift","IA","Edge","OpenShift AI","BootC","MicroShift"]},{"location":"OpenShift/Intelligence-Artificielle/EdgeIA/#4-deployer-executer-orchestrer-avec-bootc-et-redhat-device-edge","title":"4. D\u00e9ployer, Ex\u00e9cuter, Orchestrer avec BootC et RedHat Device Edge","text":"<p>Votre stack micro-service valid\u00e9e, vous allez souhaiter la d\u00e9ployer sur vos composants Edge, un Nvidia Jetson par exemple. Une solution de choix est RedHat Device Edge qui int\u00e8gre MicroShift pour orchestrer l'ensemble de vos micro-services ainsi que la stabilit\u00e9 et la large compatibilit\u00e9 de RHEL permettant de profiter dans les meilleurs conditions du GPU int\u00e9gr\u00e9 de Nvidia.</p> <p>Si vous souhaitez vous simplifier les d\u00e9ploiements et les mises \u00e0 jour, il est possible de coupler RedHat Device Edge avec BootC permettant de cr\u00e9er des images de conteneurs bootables de votre syst\u00e8me et de g\u00e9rer facilement le d\u00e9ploiement et la mise \u00e0 jour \u00e0 l'aide de votre registry interne.</p>","tags":["OpenShift","IA","Edge","OpenShift AI","BootC","MicroShift"]},{"location":"OpenShift/Intelligence-Artificielle/EdgeIA/#conclusion","title":"Conclusion","text":"<p>L'Edge IA repr\u00e9sente une avanc\u00e9e majeure dans le domaine de l'intelligence artificielle, offrant des solutions innovantes pour traiter les donn\u00e9es localement, r\u00e9duire la latence et am\u00e9liorer la s\u00e9curit\u00e9. Dans l'\u00e9cosyst\u00e8me RedHat, cette technologie trouve un terrain fertile pour se d\u00e9velopper et se d\u00e9ployer efficacement.</p> <p>Gr\u00e2ce \u00e0 des outils comme OpenShift AI, les d\u00e9veloppeurs et les data scientists peuvent entra\u00eener, affiner et d\u00e9ployer leurs mod\u00e8les de mani\u00e8re int\u00e9gr\u00e9e et efficace. OpenShift et Tekton permettent ensuite de compiler, tester et d\u00e9ployer des micro-services, assurant ainsi une cha\u00eene CI/CD robuste et fiable. Enfin, avec RedHat Device Edge et BootC, le d\u00e9ploiement et l'orchestration des applications Edge deviennent plus simples et plus s\u00e9curis\u00e9s, permettant une gestion optimale des ressources et des mises \u00e0 jour.</p> <p>En adoptant ces technologies, les entreprises peuvent non seulement am\u00e9liorer leurs op\u00e9rations et r\u00e9duire leurs co\u00fbts, mais aussi ouvrir la voie \u00e0 de nouvelles innovations dans des domaines tels que les v\u00e9hicules autonomes, les usines intelligentes et la robotique. L'Edge IA, coupl\u00e9e \u00e0 l'\u00e9cosyst\u00e8me RedHat, offre ainsi une plateforme puissante pour transformer les donn\u00e9es en actions intelligentes et en temps r\u00e9el.</p> <p>L'avenir de l'Edge IA s'annonce prometteur, et avec les outils et les technologies disponibles aujourd'hui, les possibilit\u00e9s sont presque illimit\u00e9es. Il est temps pour les entreprises de saisir ces opportunit\u00e9s et de se positionner \u00e0 l'avant-garde de cette r\u00e9volution technologique.</p> <p>Auteur : Romain GASQUET</p>","tags":["OpenShift","IA","Edge","OpenShift AI","BootC","MicroShift"]},{"location":"OpenShift/Intelligence-Artificielle/ObjectDetection/","title":"Entra\u00eenement et \u00e9valuation d'un mod\u00e8le YOLO","text":"","tags":["OpenShift","IA","Computer Vision","Object Detection","YOLO"]},{"location":"OpenShift/Intelligence-Artificielle/ObjectDetection/#introduction","title":"Introduction","text":"<p>YOLO (You Only Look Once) est un mod\u00e8le populaire pour la d\u00e9tection d'objets en temps r\u00e9el. Il est largement utilis\u00e9 pour sa rapidit\u00e9 et sa pr\u00e9cision. Cet article vous guidera \u00e0 travers les \u00e9tapes pour entra\u00eener et \u00e9valuer un mod\u00e8le YOLO.</p>","tags":["OpenShift","IA","Computer Vision","Object Detection","YOLO"]},{"location":"OpenShift/Intelligence-Artificielle/ObjectDetection/#prerequis","title":"Pr\u00e9requis","text":"<p>Avant de commencer, assurez-vous de remplir les conditions suivantes :</p> <ul> <li>\u00catre en mesure de se connecter \u00e0 un Jupyter NoteBook ayant acc\u00e8s \u00e0 un GPU.</li> <li>Cloner le d\u00e9p\u00f4t suivant pour disposer des fichiers n\u00e9cessaires :</li> </ul> <pre><code>git clone https://github.com/neutron-IT-organization/ai-demo.git\n</code></pre>","tags":["OpenShift","IA","Computer Vision","Object Detection","YOLO"]},{"location":"OpenShift/Intelligence-Artificielle/ObjectDetection/#1-creer-son-dataset","title":"1. Cr\u00e9er son dataset","text":"<p>Pour constituer votre dataset vous pouvez :</p> <ul> <li>T\u00e9l\u00e9charger un dataset opensource tel que COCO ou Pascal VOC.</li> <li>Cr\u00e9er votre propre dataset.</li> </ul> <p>Pour la cr\u00e9ation de votre propre dataset, des outils comme Label Studio peuvent \u00eatre utilis\u00e9s pour annoter les images pertinentes pour votre projet. Dans cet exemple, nous utiliserons des images de basket.</p> <p></p> <p>Une fois les images annot\u00e9es, vous pouvez exporter le mod\u00e8le au format YOLO avec les images. Pour cet article, le dataset sera stock\u00e9 dans un bucket MinIO.</p>","tags":["OpenShift","IA","Computer Vision","Object Detection","YOLO"]},{"location":"OpenShift/Intelligence-Artificielle/ObjectDetection/#2-importer-son-dataset-dans-son-environnement-de-travail","title":"2. Importer son dataset dans son environnement de travail","text":"<p>Vous pouvez maintenant utiliser le fichier <code>import-dataset.ipynb</code> pour importer votre dataset dans votre environnement de travail ainsi que le s\u00e9parer en trois sous-ensembles : </p> <ul> <li>un d'entra\u00eenement</li> <li>un d'\u00e9valuation</li> <li>un de test</li> </ul> <p>Vous pouvez sp\u00e9cifier le chemin d'acc\u00e8s \u00e0 votre dataset au sein de votre bucket, ainsi que le chemin de destination dans votre environnement de travail \u00e0 l'aide des variables DATASET_PATH et LOCAL_DATASET_PATH.</p> <p></p>","tags":["OpenShift","IA","Computer Vision","Object Detection","YOLO"]},{"location":"OpenShift/Intelligence-Artificielle/ObjectDetection/#3-entrainer-le-modele-sur-son-dataset","title":"3. Entra\u00eener le mod\u00e8le sur son dataset","text":"<p>Vous pouvez maintenant utiliser le fichier <code>transfer-learning.ipynb</code> pour entra\u00eener votre mod\u00e8le sur votre dataset.</p> <p>Dans notre exemple, nous utilisons YOLO11n que vous pouvez retrouver sur le site l'Utralytics : docs.ultralytics.com/tasks/detect/#models. </p> <p>Pour obtenir le mod\u00e8le qui correspond le mieux \u00e0 vos exigences, vous pouvez modifier les param\u00e8tres d'entra\u00eenement :</p> <ul> <li>IMGSZ : La taille des images utilis\u00e9es pour l'entra\u00eenement.</li> <li>NB_FROZEN_LAYER : Le nombre de couches gel\u00e9es pendant le transfert de l'apprentissage.</li> <li>EPOCHS : Le nombre d'it\u00e9rations d'entra\u00eenement.</li> </ul> <p>Afin d'entra\u00eener votre mod\u00e8le sur votre dataset, vous devez avoir un fichier <code>data.yaml</code> qui d\u00e9crit votre dataset. Voici un exemple de fichier : </p> <pre><code>names:\n  0: Ball\n  1: Hoop\npath: ../dataset\ntest: images/test\ntrain: images/train\nval: images/val\n</code></pre> <p></p>","tags":["OpenShift","IA","Computer Vision","Object Detection","YOLO"]},{"location":"OpenShift/Intelligence-Artificielle/ObjectDetection/#4-comparer-les-performances-des-modeles","title":"4. Comparer les performances des mod\u00e8les","text":"<p>Vous pouvez maintenant utiliser le fichier <code>comparaison.ipynb</code> pour comparer votre mod\u00e8le au mod\u00e8le de base. Nous pouvons analyser les r\u00e9sultats \u00e0 l'aide des valeurs suivantes :</p> <ul> <li>P (Pr\u00e9cision) : Mesure la proportion des d\u00e9tections positives qui sont correctes. Une valeur proche de 1 est meilleure.</li> <li>R (Rappel) : Mesure la proportion des objets pertinents d\u00e9tect\u00e9s correctement. Une valeur proche de 1 est meilleure.</li> <li>mAP50 : La pr\u00e9cision moyenne calcul\u00e9e \u00e0 un seuil de chevauchement de 50%. Une valeur proche de 1 est meilleure.</li> <li>mAP50-95 : La pr\u00e9cision moyenne calcul\u00e9e sur diff\u00e9rents seuils de chevauchement, allant de 50% \u00e0 95% avec un pas de 5%. Une valeur proche de 1 est meilleure.</li> </ul> <p></p> <p>De mani\u00e8re plus visuel, voici le r\u00e9sultat que cela donne : </p>","tags":["OpenShift","IA","Computer Vision","Object Detection","YOLO"]},{"location":"OpenShift/Intelligence-Artificielle/ObjectDetection/#5-changer-les-parametres-et-iterer","title":"5. Changer les param\u00e8tres et it\u00e9rer","text":"<p>N'h\u00e9sitez pas \u00e0 exp\u00e9rimenter avec diff\u00e9rents param\u00e8tres pour trouver le mod\u00e8le qui r\u00e9pond le mieux \u00e0 vos besoins. Documentez vos r\u00e9sultats pour chaque it\u00e9ration afin de pouvoir comparer les performances.</p>","tags":["OpenShift","IA","Computer Vision","Object Detection","YOLO"]},{"location":"OpenShift/Intelligence-Artificielle/ObjectDetection/#conclusion","title":"Conclusion","text":"<p>Cet article vous a guid\u00e9 \u00e0 travers les \u00e9tapes essentielles pour entra\u00eener et \u00e9valuer un mod\u00e8le YOLO. Vous avez appris \u00e0 cr\u00e9er et importer un dataset, entra\u00eener le mod\u00e8le, comparer ses performances, et it\u00e9rer pour am\u00e9liorer les r\u00e9sultats. La cl\u00e9 pour obtenir un bon mod\u00e8le r\u00e9side dans l'exp\u00e9rimentation et l'it\u00e9ration continue. Nous vous encourageons \u00e0 essayer diff\u00e9rentes configurations et \u00e0 observer comment chaque changement affecte les performances de votre mod\u00e8le.</p> <p>Auteur : Romain GASQUET</p>","tags":["OpenShift","IA","Computer Vision","Object Detection","YOLO"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/","title":"Configuration de GPU sur OpenShift","text":"","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#introduction","title":"Introduction","text":"<p>Dans le monde de l'informatique moderne, l'optimisation des ressources est cruciale pour maximiser les performances et r\u00e9duire les co\u00fbts. Les GPU (Graphics Processing Units) jouent un r\u00f4le central dans de nombreuses applications, allant de l'intelligence artificielle au rendu graphique. Cependant, leur utilisation efficace dans un environnement de conteneurs comme OpenShift peut \u00eatre un d\u00e9fi. Ce document vous guidera \u00e0 travers les diff\u00e9rentes m\u00e9thodes de partage de GPU sur OpenShift, en mettant l'accent sur le Time Slicing, et vous montrera comment le configurer pour am\u00e9liorer l'utilisation des ressources GPU.</p> <p></p>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#pourquoi-partager-les-gpu","title":"Pourquoi Partager les GPU ?","text":"<p>Les GPU sont des ressources co\u00fbteuses et puissantes. Dans un cluster OpenShift, il est souvent n\u00e9cessaire de partager ces ressources entre plusieurs workloads pour maximiser leur utilisation. Voici les principales m\u00e9thodes de partage de GPU :</p>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#mig-multi-instance-gpu","title":"MIG (Multi-Instance GPU)","text":"<p>MIG permet de diviser un GPU physique en plusieurs instances plus petites, chacune avec ses propres ressources d\u00e9di\u00e9es. Cela permet \u00e0 plusieurs workloads de s'ex\u00e9cuter simultan\u00e9ment sur le m\u00eame GPU, am\u00e9liorant ainsi l'utilisation des ressources. Cependant, cette m\u00e9thode n\u00e9cessite un mat\u00e9riel compatible et peut \u00eatre complexe \u00e0 configurer.</p>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#vgpu-avec-openshift-virtualization","title":"vGPU avec OpenShift Virtualization","text":"<p>vGPU virtualise un GPU physique et le partage entre plusieurs machines virtuelles (VM). Chaque VM re\u00e7oit une partie des ressources du GPU, permettant une utilisation plus flexible. Cette m\u00e9thode est id\u00e9ale pour les environnements o\u00f9 les workloads n\u00e9cessitent une isolation compl\u00e8te des ressources GPU.</p>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#time-slicing-pour-openshift","title":"Time Slicing pour OpenShift","text":"<p>Time Slicing permet de partager un GPU entre plusieurs pods en allouant des tranches de temps d'ex\u00e9cution \u00e0 chaque pod. Contrairement \u00e0 MIG et vGPU, Time Slicing ne partitionne pas les ressources GPU mais permet une utilisation concurrente en r\u00e9partissant le temps d'ex\u00e9cution. Cette m\u00e9thode est particuli\u00e8rement utile pour les workloads qui n'ont pas besoin d'une utilisation continue du GPU.</p>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#configuration-du-mode-time-slicing","title":"Configuration du Mode Time Slicing","text":"","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#etape-1-deploiement-dun-job-de-test","title":"\u00c9tape 1 : D\u00e9ploiement d'un Job de Test","text":"<p>Pour commencer, d\u00e9ployons un job de test pour v\u00e9rifier que le Time Slicing n'est pas encore configur\u00e9. Ce job ne fonctionnera pas correctement car le Time Slicing n'est pas activ\u00e9.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: dcgm-prof-tester\nspec:\n  parallelism: 4\n  template:\n    metadata:\n      labels:\n        app: dcgm-prof-tester\n    spec:\n      restartPolicy: OnFailure\n      containers:\n        - name: dcgmproftester12\n          image: nvcr.io/nvidia/cloud-native/dcgm:3.3.8-1-ubuntu22.04\n          command: [\"/usr/bin/dcgmproftester12\"]\n          args: [\"--no-dcgm-validation\", \"-t 1004\", \"-d 30\"]\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n          securityContext:\n            capabilities:\n              add: [\"SYS_ADMIN\"]\n</code></pre> <p>Appliquez ce job sur OpenShift :</p> <pre><code>oc apply -f dcgm-prof-tester.yaml\n</code></pre> <p></p> <p>R\u00e9sultat attendu :</p> <p>Si vous n'avez q'un seul GPU vous devez voir qu'un seul job fonctionne et que les 3 autres echouent.</p>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#etape-2-recuperation-du-nom-du-gpu","title":"\u00c9tape 2 : R\u00e9cup\u00e9ration du Nom du GPU","text":"<p>Stockez le nom de votre GPU dans une variable d'environnement :</p> <pre><code>GPU_NAME=$(oc get node -o jsonpath='{.items[*].metadata.labels.nvidia\\.com/gpu\\.product}')\n</code></pre>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#etape-3-creation-de-la-configmap","title":"\u00c9tape 3 : Cr\u00e9ation de la ConfigMap","text":"<p>Cr\u00e9ez une ConfigMap pour configurer le Time Slicing. Cette ConfigMap sp\u00e9cifie les param\u00e8tres de partage pour le GPU.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: device-plugin-config\n  namespace: nvidia-gpu-operator\ndata:\n  ${GPU_NAME}: |-\n    version: v1\n    sharing:\n      timeSlicing:\n        renameByDefault: false\n        resources:\n          - name: nvidia.com/gpu\n            replicas: 8\n</code></pre> <p>Ici avec \"replicas: 8\" on doit avoir 8 pod capable d'untiliser 1 gpu en mode time-slicing.</p>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#etape-4-patch-du-gpu-operator","title":"\u00c9tape 4 : Patch du GPU Operator","text":"<p>Appliquez la ConfigMap au GPU Operator pour activer le Time Slicing.</p> <pre><code>oc patch clusterpolicy gpu-cluster-policy \\\n    -n nvidia-gpu-operator --type merge \\\n    -p '{\"spec\": {\"devicePlugin\": {\"config\": {\"name\": \"device-plugin-config\"}}}}'\n</code></pre> <p>R\u00e9sultat attendu :</p> <pre><code>clusterpolicy.nvidia.com/gpu-cluster-policy patched\n</code></pre>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#etape-5-configuration-du-node","title":"\u00c9tape 5 : Configuration du Node","text":"<p>Appliquez ensuite le label correspondant :</p> <pre><code>oc label --overwrite node \\\n    --selector=nvidia.com/gpu.product=${GPU_NAME} \\\n    nvidia.com/device-plugin.config=${GPU_NAME}\n</code></pre> <p>R\u00e9sultat attendu :</p> <pre><code>node/&lt;node-name&gt; labeled\n</code></pre>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#etape-6-validation-de-la-configuration","title":"\u00c9tape 6 : Validation de la Configuration","text":"<p>V\u00e9rifiez que la capacit\u00e9 du GPU a bien \u00e9t\u00e9 mise \u00e0 jour :</p> <pre><code>oc get node --selector=nvidia.com/gpu.product=${GPU_NAME}-SHARED -o json | jq '.items[0].status.capacity'\n</code></pre> <p>Exemple R\u00e9sultat :</p> <pre><code>{\n  \"cpu\": \"28\",\n  \"devices.kubevirt.io/kvm\": \"1k\",\n  \"devices.kubevirt.io/tun\": \"1k\",\n  \"devices.kubevirt.io/vhost-net\": \"1k\",\n  \"ephemeral-storage\": \"975760576Ki\",\n  \"hugepages-1Gi\": \"0\",\n  \"hugepages-2Mi\": \"0\",\n  \"memory\": \"131711436Ki\",\n  \"nvidia.com/gpu\": \"8\",\n  \"pods\": \"250\"\n}\n</code></pre> <p>On observe notamment que la configuraiton \"nvidia.com/gpu\": \"8\" est maintenant appliqu\u00e9.</p>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#etape-7-relancer-le-job-de-test","title":"\u00c9tape 7 : Relancer le Job de Test","text":"<p>Relancez le job de test pour v\u00e9rifier que le Time Slicing est bien activ\u00e9.</p> <pre><code>oc delete job dcgm-prof-tester\noc apply -f dcgm-prof-tester.yaml\n</code></pre> <p>R\u00e9sultat attendu :</p> <p>Le job doit s'ex\u00e9cuter correctement avec plusieurs pods partageant le GPU sans erreur.</p> <p></p>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/SharedGPU/#conclusion","title":"Conclusion","text":"<p>En configurant le mode Time Slicing, vous optimisez l'utilisation des ressources GPU dans votre cluster OpenShift. Cette m\u00e9thode permet de partager efficacement les GPU entre plusieurs workloads, am\u00e9liorant ainsi les performances globales et r\u00e9duisant les co\u00fbts. Dans un prochain blogpost, nous explorerons comment configurer OpenShift avec vGPU pour une utilisation encore plus flexible des ressources GPU.</p> <p>Auteur : Florian EVEN</p>","tags":["OpenShift","IA","GPU"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/","title":"Rendre un Mod\u00e8le Disponible en Inf\u00e9rence sur OpenShift avec Llama.cpp","text":"","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#introduction","title":"Introduction","text":"<p>Dans cet article, nous allons explorer comment d\u00e9ployer un mod\u00e8le de langage pour l'inf\u00e9rence sur OpenShift en utilisant Llama.cpp. Notre architecture globale comprendra un serveur MinIO pour rendre les mod\u00e8les accessibles via S3, un pod Kubernetes avec un init container pour t\u00e9l\u00e9charger le mod\u00e8le, et un conteneur principal ex\u00e9cutant Llama.cpp pour rendre le mod\u00e8le disponible pour l'inf\u00e9rence. Enfin, nous d\u00e9ployerons une interface utilisateur pour interagir avec le mod\u00e8le.</p>","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#prerequis","title":"Pr\u00e9requis","text":"<p>Avant de commencer, assurez-vous d'avoir les \u00e9l\u00e9ments suivants :</p> <ol> <li>Un cluster OpenShift : Assurez-vous d'avoir acc\u00e8s \u00e0 un cluster OpenShift.</li> <li>MinIO : Un serveur MinIO configur\u00e9 pour stocker vos mod\u00e8les.</li> <li>Outils CLI : <code>oc</code> (OpenShift CLI) et <code>kubectl</code> pour interagir avec votre cluster.</li> </ol> <p>NOTE: Toute les actions ci-dessous seront r\u00e9alis\u00e9es dans le namespace myllm. Si vous utilisez un autres namespace mettez \u00e0 jour les URL/namespace en cons\u00e9quence.</p>","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#partie-1-telechargement-et-upload-du-modele","title":"Partie 1 : T\u00e9l\u00e9chargement et Upload du Mod\u00e8le","text":"","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#telecharger-mistral-depuis-hugging-face","title":"T\u00e9l\u00e9charger Mistral depuis Hugging Face","text":"<p>Pour commencer, t\u00e9l\u00e9chargez le mod\u00e8le de votre choix depuis Hugging Face. Dans notre tutoriel nous utiliserons Mistral-7B. Vous pouvez utiliser la commande suivante pour t\u00e9l\u00e9charger le mod\u00e8le au format GGUF.</p> <pre><code># Exemple de commande pour t\u00e9l\u00e9charger un mod\u00e8le\nwget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf?download=true\n</code></pre> <p></p>","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#upload-du-modele-dans-minio","title":"Upload du Mod\u00e8le dans MinIO","text":"<p>Une fois le mod\u00e8le t\u00e9l\u00e9charg\u00e9 et converti, vous devez l'uploader dans votre bucket MinIO. Utilisez l'interface utilisateur de MinIO ou l'outil de ligne de commande <code>mc</code> pour uploader le mod\u00e8le.</p> <pre><code>mc cp /path/to/your/model/mistral-7b-instruct-v0.2.Q4_K_M.gguf myminio/mybucket/models/\n</code></pre> <p></p>","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#partie-2-deploiement-du-pod-llamacpp","title":"Partie 2 : D\u00e9ploiement du Pod Llama.cpp","text":"","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#configuration-du-pod","title":"Configuration du Pod","text":"<p>Nous allons d\u00e9ployer un pod Kubernetes avec un init container pour t\u00e9l\u00e9charger le mod\u00e8le depuis MinIO et un conteneur principal pour ex\u00e9cuter Llama.cpp.</p> <p>Voici un exemple de fichier YAML pour le d\u00e9ploiement :</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simplified-deployment\n  namespace: myllm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: simplified-app\n  template:\n    metadata:\n      labels:\n        app: simplified-app\n    spec:\n      volumes:\n        - name: models-volume\n          emptyDir: {}\n        - name: shm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 2Gi\n      initContainers:\n        - name: s3-copy\n          image: appropriate/curl\n          command:\n            - sh\n            - '-c'\n            - 'curl -k -o /mnt/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf https://minio-s3-myllm.apps.neutron-sno-gpu.neutron-it.fr/models/mistral7bv2/mistral-7b-instruct-v0.2.Q4_K_M.gguf'\n          volumeMounts:\n            - name: models-volume\n              mountPath: /mnt/models\n      containers:\n        - name: main-container\n          image: quay.io/neutron-it/custom-model-serving-runtime:latest\n          env:\n            - name: MODEL_PATH\n              value: /mnt/models\n            - name: HOST\n              value: 0.0.0.0\n            - name: PORT\n              value: '8080'\n          ports:\n            - containerPort: 8080\n              protocol: TCP\n          volumeMounts:\n            - name: models-volume\n              mountPath: /mnt/models\n            - name: shm\n              mountPath: /dev/shm\n          securityContext:\n            runAsUser: 0\n</code></pre>","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#explication","title":"Explication","text":"<ul> <li>Init Container : T\u00e9l\u00e9charge le mod\u00e8le depuis MinIO et le place dans un volume partag\u00e9.</li> <li>Conteneur Principal : Ex\u00e9cute Llama.cpp pour rendre le mod\u00e8le disponible pour l'inf\u00e9rence.</li> </ul>","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#service-et-route","title":"Service et Route","text":"<p>Pour exposer le service, vous aurez besoin d'un Service et d'une Route :</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: simplified-service\n  namespace: myllm\nspec:\n  selector:\n    app: simplified-app\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: simplified-route\n  namespace: myllm\nspec:\n  to:\n    kind: Service\n    name: simplified-service\n  port:\n    targetPort: 8080\n  tls:\n    termination: edge\n    insecureEdgeTerminationPolicy: Redirect\n</code></pre>","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#test-du-service","title":"Test du Service","text":"<p>Vous pouvez tester le service en utilisant <code>curl</code> depuis le pod cr\u00e9\u00e9:</p> <pre><code>curl --location 'https://localhost:8080/v1/chat/completions' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"messages\": [\n    {\n      \"content\": \"You are a helpful assistant.\",\n      \"role\": \"system\"\n    },\n    {\n      \"content\": \"How large is the capital of France?\",\n      \"role\": \"user\"\n    }\n  ]\n}'\n</code></pre> <p></p>","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#partie-3-deploiement-de-linterface-utilisateur","title":"Partie 3 : D\u00e9ploiement de l'Interface Utilisateur","text":"<p>Pour d\u00e9ployer l'interface utilisateur, utilisez le fichier YAML suivant :</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: chatbot-ui\n  namespace: myllm\n  labels:\n    app: chatbot-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: chatbot-ui\n  template:\n    metadata:\n      labels:\n        app: chatbot-ui\n    spec:\n      containers:\n        - name: chatbot-ui\n          image: 'quay.io/neutron-it/chatbot-ui:latest'\n          ports:\n            - containerPort: 8080\n              protocol: TCP\n            - containerPort: 8501\n              protocol: TCP\n          env:\n            - name: MODEL_ENDPOINT\n              value: 'http://simplified-service.myllm.svc.cluster.local:8080'\n          resources: {}\n          imagePullPolicy: Always\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n      securityContext: {}\n      schedulerName: default-scheduler\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n  revisionHistoryLimit: 10\n  progressDeadlineSeconds: 600\n</code></pre>","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#service-et-route-pour-lui","title":"Service et Route pour l'UI","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: chatbot-ui\n  namespace: myllm\nspec:\n  ports:\n    - name: 8080-tcp\n      protocol: TCP\n      port: 8080\n      targetPort: 8080\n    - name: 8501-tcp\n      protocol: TCP\n      port: 8501\n      targetPort: 8501\n  selector:\n    app: chatbot-ui\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: chatbot-ui\n  namespace: myllm\nspec:\n  to:\n    kind: Service\n    name: chatbot-ui\n  port:\n    targetPort: 8501-tcp\n  tls:\n    termination: edge\n    insecureEdgeTerminationPolicy: Redirect\n</code></pre>","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/llamaCPP/#exemple-de-question","title":"Exemple de Question","text":"<p>Une fois l'interface utilisateur d\u00e9ploy\u00e9e, vous pouvez poser des questions comme :</p> <ul> <li>\"Quelle est la capitale de la France ?\"</li> <li>\"Peux-tu m'expliquer la th\u00e9orie de la relativit\u00e9 ?\"</li> <li>\"Quelle est la taille de Paris ?\"</li> </ul> <p>Auteur : Florian EVEN</p>","tags":["OpenShift","IA","Llama","Openshift","IA"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/","title":"Mise en place d'un RAG d\u00e9connect\u00e9 sur OpenShift AI","text":"","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#introduction","title":"Introduction","text":"<p>Le RAG (Retrieval-Augmented Generation) est une technique puissante qui permet de connecter un Grand Mod\u00e8le de Langage (LLM) \u00e0 une base de connaissance externe. Cela permet au mod\u00e8le de fournir des r\u00e9ponses pr\u00e9cises et factuelles bas\u00e9es sur des documents sp\u00e9cifiques, plut\u00f4t que sur ses connaissances g\u00e9n\u00e9rales.</p> <p>Cet article vous guidera \u00e0 travers les \u00e9tapes pour construire, sur la plateforme OpenShift AI, un syst\u00e8me RAG complet et d\u00e9connect\u00e9 d'Internet, en utilisant le mod\u00e8le Llama 3.2, des librairies open-source comme LangChain, et un stockage S3 compatible MinIO.</p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#code-source","title":"Code Source","text":"<p>Voici le d\u00e9p\u00f4t GitHub qui contient tout le code utilis\u00e9 pour l'impl\u00e9mentation de ce RAG. Vous y trouverez le notebook Jupyter complet, pr\u00eat \u00e0 \u00eatre ex\u00e9cut\u00e9.</p> <p>Acc\u00e9der au notebook sur GitHub</p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#1-preparation-de-lenvironnement-et-des-modeles","title":"1. Pr\u00e9paration de l'Environnement et des Mod\u00e8les","text":"<p>La premi\u00e8re phase consiste \u00e0 r\u00e9cup\u00e9rer tous les composants n\u00e9cessaires et \u00e0 les centraliser dans notre entrep\u00f4t de donn\u00e9es MinIO.</p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#11-telechargement-des-modeles-depuis-hugging-face","title":"1.1. T\u00e9l\u00e9chargement des Mod\u00e8les depuis Hugging Face","text":"<p>Avant toute chose, nous devons t\u00e9l\u00e9charger les mod\u00e8les qui seront au c\u0153ur de notre syst\u00e8me. La m\u00e9thode la plus simple et la plus robuste est de cloner directement leurs d\u00e9p\u00f4ts Git. Pour ce projet, nous utilisons :</p> <p><code>meta-llama/Llama-3.2-3B-Instruct</code> : Le LLM qui g\u00e9n\u00e9rera les r\u00e9ponses. <code>sentence-transformers/all-MiniLM-L6-v2</code> : Le mod\u00e8le d'embedding qui transformera notre texte en vecteurs. Pour ce faire, vous devrez suivre quelques \u00e9tapes de configuration la premi\u00e8re fois.</p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#etape-1-obtenir-un-jeton-dacces-token-hugging-face","title":"\u00c9tape 1 : Obtenir un Jeton d'Acc\u00e8s (Token) Hugging Face","text":"<p>Le mod\u00e8le Llama 3.2 est un mod\u00e8le \u00e0 acc\u00e8s contr\u00f4l\u00e9 (\"gated model\"). Pour le t\u00e9l\u00e9charger, vous devez accepter ses conditions d'utilisation sur sa page Hugging Face et vous authentifier avec un jeton personnel.</p> <ul> <li>Connectez-vous \u00e0 votre compte sur huggingface.co.</li> <li>Allez sur votre Profil -&gt; Settings -&gt; Access Tokens.</li> <li>Cliquez sur \"New token\", donnez-lui un nom (ex: <code>openshift-downloader</code>) et assignez-lui un r\u00f4le <code>read</code>.</li> <li>Copiez le jeton g\u00e9n\u00e9r\u00e9 (il commence par <code>hf_...</code>).</li> </ul>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#etape-2-sauthentifier-dans-le-terminal","title":"\u00c9tape 2 : S'authentifier dans le Terminal","text":"<p>Pour que Git puisse utiliser votre jeton, vous devez vous connecter via le terminal de votre machine.</p> <ul> <li>Installez l'interface en ligne de commande de Hugging Face si ce n'est pas d\u00e9j\u00e0 fait :</li> </ul> <pre><code>pip install -U \"huggingface_hub[cli]\"\n</code></pre> <ul> <li>Lancez la commande de connexion :</li> </ul> <pre><code>huggingface-cli login\n</code></pre> <ul> <li>Collez votre jeton d'acc\u00e8s lorsque vous y \u00eates invit\u00e9 et appuyez sur Entr\u00e9e.</li> </ul>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#etape-3-cloner-les-depots-des-modeles","title":"\u00c9tape 3 : Cloner les D\u00e9p\u00f4ts des Mod\u00e8les","text":"<p>Maintenant que tout est configur\u00e9, vous pouvez cloner les deux d\u00e9p\u00f4ts avec les commandes suivantes.</p> <pre><code># Cloner le mod\u00e8le Llama 3.2 Instruct\ngit clone https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n\n# Cloner le mod\u00e8le d'embedding\ngit clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n</code></pre>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#12-centralisation-sur-minio","title":"1.2. Centralisation sur MinIO","text":"<p>Une fois les mod\u00e8les et vos documents sources (PDF, etc.) sur votre machine, l'\u00e9tape suivante est de les t\u00e9l\u00e9verser sur votre serveur MinIO. Cela cr\u00e9e une source de donn\u00e9es unique et fiable pour notre application, la rendant ind\u00e9pendante de l'environnement d'ex\u00e9cution.</p> <p>Cr\u00e9ez un bucket (par exemple <code>neutronIT-rag</code>) et organisez vos fichiers avec des pr\u00e9fixes clairs.</p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#2-initialisation-du-notebook-et-connexion-aux-donnees","title":"2. Initialisation du Notebook et Connexion aux Donn\u00e9es","text":"<p>Nous passons maintenant au notebook Jupyter qui orchestre le RAG. </p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#21-installation-des-dependances","title":"2.1. Installation des D\u00e9pendances","text":"<p>Avant toute chose, ex\u00e9cutez la cellule suivante dans votre notebook pour installer toutes les librairies Python requises. Cette commande t\u00e9l\u00e9charge et installe l'ensemble des outils n\u00e9cessaires au fonctionnement de notre RAG.</p> <pre><code>!pip install huggingface_hub transformers accelerate sentencepiece bitsandbytes langchain langchain-community sentence-transformers chromadb pypdf unstructured pysqlite3-binary langchain-experimental minio\n</code></pre>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#22-assurer-le-fonctionnement-hors-ligne-nltk","title":"2.2. Assurer le fonctionnement hors ligne (NLTK)","text":"<p>Pour que notre application soit v\u00e9ritablement d\u00e9connect\u00e9e, nous devons nous assurer que toutes ses d\u00e9pendances sont disponibles localement. L'outil SemanticChunker, que nous utiliserons plus tard, d\u00e9pend de la librairie NLTK qui, par d\u00e9faut, t\u00e9l\u00e9charge des paquets de donn\u00e9es depuis Internet.</p> <p>Le code ci-dessous r\u00e9sout ce probl\u00e8me. Il doit \u00eatre ex\u00e9cut\u00e9 une premi\u00e8re fois avec une connexion Internet. Il va t\u00e9l\u00e9charger les paquets punkt (pour la ponctuation) et stopwords (mots courants) dans un dossier local ./nltk_data. Lors des ex\u00e9cutions suivantes, m\u00eame sans Internet, le script utilisera ce dossier local.</p> <pre><code>import os\nimport nltk\nimport ssl\n\nnltk_data_dir = \"./nltk_data\"\n\nif not os.path.exists(nltk_data_dir):\n    print(f\"Le dossier '{nltk_data_dir}' est introuvable.\")\n    print(\"Tentative de t\u00e9l\u00e9chargement des paquets NLTK requis ('punkt', 'stopwords')...\")\n\n    os.makedirs(nltk_data_dir)\n\n    try:\n        _create_unverified_https_context = ssl._create_unverified_context\n    except AttributeError:\n        pass\n    else:\n        ssl._create_default_https_context = _create_unverified_https_context\n\n    nltk.download('punkt', download_dir=nltk_data_dir)\n    nltk.download('stopwords', download_dir=nltk_data_dir)\n\n    print(\"T\u00e9l\u00e9chargement des paquets NLTK termin\u00e9.\")\nelse:\n    print(f\"Le dossier '{nltk_data_dir}' existe d\u00e9j\u00e0. Aucune action de t\u00e9l\u00e9chargement n'est n\u00e9cessaire.\")\n\nif os.path.abspath(nltk_data_dir) not in nltk.data.path:\n    nltk.data.path.append(os.path.abspath(nltk_data_dir))\n\nprint(\"Le script est configur\u00e9 pour utiliser les paquets NLTK en mode d\u00e9connect\u00e9.\")\n\n</code></pre>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#23-connexion-a-minio-et-recuperation-des-modeles","title":"2.3. Connexion \u00e0 MinIO et R\u00e9cup\u00e9ration des Mod\u00e8les","text":"<p>Maintenant que l'environnement est pr\u00eat, nous devons \u00e9tablir la connexion entre notre notebook et le serveur MinIO. Pour des raisons de s\u00e9curit\u00e9, les identifiants (cl\u00e9s d'acc\u00e8s) ne sont jamais \u00e9crits directement dans le code. Sur OpenShift AI, la m\u00e9thode recommand\u00e9e est d'utiliser une \"Connexion de donn\u00e9es\", qui stocke ces informations de mani\u00e8re s\u00e9curis\u00e9e et les rend disponibles pour votre workbench.</p> <p>Configuration via l'Interface d'OpenShift AI La connexion se fait en deux temps, directement dans l'interface graphique :</p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#231-creer-une-connexion-de-donnees","title":"2.3.1 Cr\u00e9er une Connexion de Donn\u00e9es :","text":"<p>Dans votre projet sur OpenShift AI, allez dans la section \"Connexions de donn\u00e9es\" et cliquez sur \"Ajouter une connexion de donn\u00e9es\". Remplissez les champs avec les informations de votre serveur MinIO :</p> <p>Nom de la connexion : Un nom descriptif (ex: Connexion MinIO Projet RAG).</p> <p>Access key ID : Votre cl\u00e9 d'acc\u00e8s MinIO.</p> <p>Secret access key : Votre cl\u00e9 secr\u00e8te MinIO.</p> <p>Endpoint : L'URL de votre service MinIO.</p> <p> </p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#232-lier-la-connexion-au-workbench","title":"2.3.2 Lier la Connexion au Workbench :","text":"<p>Lors de la configuration de votre workbench (ou en le modifiant), descendez jusqu'\u00e0 la section \"Connexions de donn\u00e9es\". S\u00e9lectionnez dans la liste d\u00e9roulante la connexion que vous venez de cr\u00e9er.</p> <p> </p> <p>En faisant cela, OpenShift AI va automatiquement injecter les identifiants de cette connexion comme variables d'environnement (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_S3_ENDPOINT) dans l'environnement de votre notebook.</p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#233-code-dutilisation-des-variables","title":"2.3.3 Code d'Utilisation des Variables :","text":"<p>Le code Python ci-dessous peut alors lire ces variables de mani\u00e8re s\u00e9curis\u00e9e gr\u00e2ce \u00e0 os.getenv(), sans jamais exposer les identifiants dans le code.</p> <pre><code>import os\nfrom minio import Minio\nfrom minio.error import S3Error\n\n# --- Configuration MinIO ---\nMINIO_BUCKET_NAME = \"neutronIT-rag\"\nAccess_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\nSecret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\ns3_endpoint = os.getenv(\"AWS_S3_ENDPOINT\")\n\n# --- Chemins locaux et MinIO ---\nLLM_MINIO_PREFIX = \"Meta-Llama-3.2-3B-Instruct/\"\nLLM_LOCAL_PATH = \"./models/Meta-Llama-3.2-3B-Instruct\"\n\nEMBEDDING_MINIO_PREFIX = \"all-MiniLM-L6-v2/\"\nEMBEDDING_LOCAL_PATH = \"./models/all-MiniLM-L6-v2\"\n\ndef download_model_via_streaming(client, bucket, prefix, local_path):\n    \"\"\"\n    T\u00e9l\u00e9charge les fichiers via streaming pour une utilisation m\u00e9moire minimale.\n    \"\"\"\n    if not os.path.exists(local_path):\n        print(f\"-&gt; Le dossier '{local_path}' n'existe pas. D\u00e9but du t\u00e9l\u00e9chargement de '{prefix}'...\")\n        os.makedirs(local_path, exist_ok=True)\n        try:\n            objects = client.list_objects(bucket, prefix=prefix, recursive=True)\n            files_to_download = [obj for obj in objects if not obj.object_name.endswith('/')]\n\n            for obj in files_to_download:\n                file_name = os.path.relpath(obj.object_name, prefix)\n                local_file_path = os.path.join(local_path, file_name)\n\n                if not os.path.exists(os.path.dirname(local_file_path)):\n                    os.makedirs(os.path.dirname(local_file_path))\n\n                response = None\n                try:\n                    response = client.get_object(bucket, obj.object_name)\n                    with open(local_file_path, 'wb') as file_data:\n                        for chunk in response.stream(amt=1024*1024):\n                            file_data.write(chunk)\n                finally:\n                    if response:\n                        response.close()\n                        response.release_conn()\n        except S3Error as exc:\n            print(f\"   Une erreur S3 est survenue pour {prefix}: {exc}\")\n            raise\n    else:\n        print(f\"-&gt; Le mod\u00e8le dans '{local_path}' existe d\u00e9j\u00e0.\")\n\n# --- Ex\u00e9cution du t\u00e9l\u00e9chargement ---\ntry:\n    minio_client = Minio(s3_endpoint, access_key=Access_key, secret_key=Secret_key, secure=False)\n    print(\"Connexion \u00e0 MinIO r\u00e9ussie.\")\n\n    download_model_via_streaming(minio_client, MINIO_BUCKET_NAME, LLM_MINIO_PREFIX, LLM_LOCAL_PATH)\n    download_model_via_streaming(minio_client, MINIO_BUCKET_NAME, EMBEDDING_MINIO_PREFIX, EMBEDDING_LOCAL_PATH)\n\nexcept Exception as e:\n    print(f\"Une erreur est survenue lors de la phase de t\u00e9l\u00e9chargement : {e}\")\n\nprint(\"\\n Tous les mod\u00e8les sont v\u00e9rifi\u00e9s et pr\u00eats localement.\")\n</code></pre> <p>-&gt; R\u00e9sultat :</p> <p> </p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#3-chargement-et-configuration-des-modeles-ia","title":"3. Chargement et Configuration des Mod\u00e8les IA","text":"<p>Avec les fichiers de mod\u00e8les disponibles localement, nous pouvons maintenant les charger en m\u00e9moire.</p> <p>Pour le LLM, une technique de quantification 4-bit (<code>BitsAndBytesConfig</code>) est utilis\u00e9e. Elle r\u00e9duit drastiquement l'empreinte m\u00e9moire du mod\u00e8le sur le GPU, le rendant utilisable sur des configurations plus modestes, sans perte de performance significative.</p> <pre><code>import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom langchain_community.embeddings import SentenceTransformerEmbeddings\n\n# 1. Configuration de la quantification 4-bit\nnf4_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\n# 2. Chargement du tokenizer du LLM\nprint(f\"Chargement du tokenizer depuis le chemin local : {LLM_LOCAL_PATH}...\")\ntokenizer = AutoTokenizer.from_pretrained(\n    LLM_LOCAL_PATH,\n    trust_remote_code=True,\n    local_files_only=True\n)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# 3. Chargement du mod\u00e8le LLM quantifi\u00e9\nprint(f\"Chargement du mod\u00e8le depuis {LLM_LOCAL_PATH} avec quantisation 4-bit...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    LLM_LOCAL_PATH,\n    quantization_config=nf4_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    local_files_only=True\n)\nprint(\"Mod\u00e8le LLM charg\u00e9 avec succ\u00e8s !\")\n\n# 4. Chargement du mod\u00e8le d'embedding\nembeddings = SentenceTransformerEmbeddings(\n    model_name=EMBEDDING_LOCAL_PATH, \n    model_kwargs={'device': 'cuda'}\n)\nprint(\"Mod\u00e8le d'embedding charg\u00e9 avec succ\u00e8s !\")\n</code></pre> <p>-&gt; R\u00e9sultats :   </p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#4-pipeline-dingestion-des-donnees-le-retrieval","title":"4. Pipeline d'Ingestion des Donn\u00e9es (Le \"Retrieval\")","text":"<p>Cette \u00e9tape transforme nos documents bruts en une base de connaissance interrogeable. Elle se d\u00e9roule en trois temps : l'extraction du texte, le d\u00e9coupage s\u00e9mantique, et enfin la vectorisation.</p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#41-chargement-et-extraction-des-documents","title":"4.1. Chargement et Extraction des Documents","text":"<p>La premi\u00e8re action consiste \u00e0 r\u00e9cup\u00e9rer les documents sources (ici, des PDF) depuis notre stockage MinIO. Le script parcourt le dossier <code>documents/</code>, t\u00e9l\u00e9charge chaque fichier localement, puis utilise la librairie <code>PyPDFLoader</code>. C'est la m\u00e9thode <code>loader.load()</code> qui se charge d'ouvrir chaque PDF et d'en extraire le contenu textuel brut, page par page.</p> <pre><code>from langchain_community.document_loaders import PyPDFLoader\n\n# 1. Chargement et extraction des documents PDF depuis MinIO\nPDF_DIRECTORY_ON_MINIO = \"documents/\"\nLOCAL_PDF_DOWNLOAD_DIR = \"./pdf_downloads/\"\nall_documents = []\nos.makedirs(LOCAL_PDF_DOWNLOAD_DIR, exist_ok=True)\n\npdf_objects = minio_client.list_objects(MINIO_BUCKET_NAME, prefix=PDF_DIRECTORY_ON_MINIO, recursive=True)\npdf_object_names = [obj.object_name for obj in pdf_objects if obj.object_name.lower().endswith('.pdf')]\n\nfor pdf_object_name in pdf_object_names:\n    local_pdf_path = os.path.join(LOCAL_PDF_DOWNLOAD_DIR, os.path.basename(pdf_object_name))\n    if not os.path.exists(local_pdf_path):\n        minio_client.fget_object(MINIO_BUCKET_NAME, pdf_object_name, local_pdf_path)\n\n    # La librairie PyPDFLoader s'occupe ici de l'extraction du texte\n    loader = PyPDFLoader(local_pdf_path)\n    documents_from_this_pdf = loader.load()\n    all_documents.extend(documents_from_this_pdf)\n\nprint(f\"Traitement termin\u00e9. Nombre total de pages pr\u00eates pour le RAG : {len(all_documents)}\")\n</code></pre> <p>-&gt; R\u00e9sultats :   </p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#42-decoupage-semantique-du-texte","title":"4.2. D\u00e9coupage S\u00e9mantique du Texte","text":"<p>Une fois le texte extrait, il est trop volumineux pour \u00eatre envoy\u00e9 tel quel \u00e0 un LLM. Nous devons le diviser en plus petits morceaux (\"chunks\"). Plut\u00f4t qu'un d\u00e9coupage arbitraire, nous utilisons <code>SemanticChunker</code>. Cet outil analyse le texte et le coupe aux endroits o\u00f9 le sens change, cr\u00e9ant ainsi des chunks plus coh\u00e9rents et pertinents pour la recherche d'informations.</p> <pre><code>from langchain_experimental.text_splitter import SemanticChunker\n\n# 2. D\u00e9coupage s\u00e9mantique\nprint(\"D\u00e9coupage des documents en chunks s\u00e9mantiques...\")\ntext_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\nall_chunks = text_splitter.split_documents(all_documents)\nprint(f\"Nombre total de chunks cr\u00e9\u00e9s : {len(all_chunks)}\")\n</code></pre> <p>-&gt; R\u00e9sultats :   </p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#43-vectorisation-et-stockage","title":"4.3. Vectorisation et Stockage","text":"<p>C'est la derni\u00e8re \u00e9tape de la pr\u00e9paration. Chaque chunk de texte est transform\u00e9 en un vecteur num\u00e9rique (un \"embedding\") \u00e0 l'aide de notre mod\u00e8le <code>all-MiniLM-L6-v2</code>. Ces vecteurs sont ensuite stock\u00e9s et index\u00e9s dans une base de donn\u00e9es vectorielle, ChromaDB, qui nous permettra de retrouver efficacement les chunks les plus pertinents par rapport \u00e0 une question.</p> <pre><code>from langchain_community.vectorstores import Chroma\n\n# 3. Vectorisation et stockage dans ChromaDB\nprint(\"Initialisation du Vector Store (ChromaDB) avec les chunks...\")\nvectorstore = Chroma.from_documents(documents=all_chunks, embedding=embeddings)\nprint(\"Vector Store pr\u00eat !\")\n</code></pre>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#5-construction-de-la-chaine-de-generation-le-augmented-generation","title":"5. Construction de la Cha\u00eene de G\u00e9n\u00e9ration (Le \"Augmented Generation\")","text":"<p>Maintenant que notre base de connaissance est pr\u00eate, nous assemblons la cha\u00eene RAG.</p> <pre><code>from langchain_core.retrievers import BaseRetriever\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_community.llms import HuggingFacePipeline\nfrom transformers import pipeline\nfrom typing import List\n\n# 1. Retriever Personnalis\u00e9 (`NeighborRetriever`)\nclass NeighborRetriever(BaseRetriever):\n    vectorstore: Chroma\n    all_docs: List\n\n    def _get_relevant_documents(self, query: str, *, run_manager) -&gt; List:\n        best_docs = self.vectorstore.similarity_search(query, k=1)\n        if not best_docs: return []\n\n        try:\n            best_doc_index = [doc.page_content for doc in self.all_docs].index(best_docs[0].page_content)\n        except ValueError:\n            return best_docs\n\n        start_index = max(0, best_doc_index - 1)\n        end_index = min(len(self.all_docs), best_doc_index + 2)\n        return self.all_docs[start_index:end_index]\n\nretriever = NeighborRetriever(vectorstore=vectorstore, all_docs=all_chunks)\nprint(\"Retriever \u00e0 contexte enrichi (voisins) pr\u00eat !\")\n\n\n# 2. Pipeline de g\u00e9n\u00e9ration et Prompt Template\ntext_generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=4096)\nllm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n\nprompt_template_str = \"\"\"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\nTu es un assistant utile. Pour r\u00e9pondre, r\u00e9f\u00e8re-toi d'abord au CONTEXTE FOURNI.\nSi la r\u00e9ponse NE SE TROUVE PAS dans le contexte, utilise tes propres connaissances.\nSi tu ne connais pas la r\u00e9ponse, dis simplement que tu ne sais pas.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\nContexte:\n{context}\n\nQuestion: {input}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\"\"\"\nprompt = ChatPromptTemplate.from_template(prompt_template_str)\n\n\n# 3. Assemblage de la cha\u00eene RAG\ndocument_chain = create_stuff_documents_chain(llm, prompt)\nretrieval_chain = create_retrieval_chain(retriever, document_chain)\nprint(\"Cha\u00eene RAG compl\u00e8te assembl\u00e9e !\")\n</code></pre>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#6-iteration-et-comparaison-des-resultats","title":"6. It\u00e9ration et Comparaison des R\u00e9sultats","text":"<p>Le syst\u00e8me est pr\u00eat. L'\u00e9tape finale du notebook lance une boucle interactive o\u00f9, pour chaque question de l'utilisateur, le script utilise d'abord le LLM pour reformuler et am\u00e9liorer la question, la rendant plus pr\u00e9cise pour la recherche. Ensuite, cette question optimis\u00e9e est envoy\u00e9e \u00e0 la fois au LLM seul pour une r\u00e9ponse g\u00e9n\u00e9rale, et \u00e0 la cha\u00eene RAG pour une r\u00e9ponse bas\u00e9e sur le contexte de nos documents. Cela permet de comparer en temps r\u00e9el la connaissance intrins\u00e8que du mod\u00e8le avec les r\u00e9ponses factuelles et pr\u00e9cises fournies par le RAG.</p> <pre><code>def transform_query_with_llm(question: str, model, tokenizer) -&gt; str:\n    transformation_prompt_template = \"\"\"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n    Tu es un outil de reformulation de requ\u00eates. Ton seul r\u00f4le est de r\u00e9\u00e9crire la question de l'utilisateur pour la rendre optimale pour une recherche s\u00e9mantique.\n    R\u00e8gles strictes:\n    1. Ne r\u00e9ponds JAMAIS \u00e0 la question.\n    2. Garde le sens original de la question.\n    3. Ta sortie doit \u00eatre une et une seule question.\n    4. La question reformul\u00e9e doit \u00eatre concise.\n\n    Exemple 1:\n    Utilisateur: infos sur la s\u00e9curit\u00e9 openshift\n    Assistant: Quelles sont les meilleures pratiques de s\u00e9curit\u00e9 pour un cluster OpenShift ?\n\n    Exemple 2:\n    Utilisateur: tu peux me faire un r\u00e9sum\u00e9 ?\n    Assistant: Quel est le r\u00e9sum\u00e9 du document fourni ?\n\n    Exemple 3:\n    Utilisateur: c'est quoi les grands titre dont tu peux m'aider, en se basant sur le contexte?\n    Assistant: Quels sont les th\u00e8mes principaux abord\u00e9s dans le document ?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n    {question}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n    \"\"\"\n    formatted_prompt = transformation_prompt_template.format(question=question)\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n    transformed_question = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n    return transformed_question.strip()\n\n\ndef get_direct_llm_answer(question, model, tokenizer):\n    messages = [{\"role\": \"user\", \"content\": question}]\n    prompt_for_direct_answer = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(prompt_for_direct_answer, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=4096, pad_token_id=tokenizer.eos_token_id)\n    answer = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n    return answer\n\n# --- Boucle d'interaction principale ---\nprint(\"\\n\\n Syst\u00e8me RAG pr\u00eat. Posez une question (tapez 'exit' pour quitter) :\")\nwhile True:\n    try:\n        prompt_text = input(\"&gt;&gt;&gt; \")\n        if prompt_text.lower() == 'exit':\n            break\n\n        print(\"\\n--- Transformation de la question par le LLM ---\")\n        transformed_query = transform_query_with_llm(prompt_text, model, tokenizer)\n        print(f\"Question transform\u00e9e : '{transformed_query}'\")\n\n        print(\"\\n--- R\u00e9ponse du Mod\u00e8le Llama (Connaissances G\u00e9n\u00e9rales) ---\")\n        answer_direct = get_direct_llm_answer(transformed_query, model, tokenizer)\n        print(f\"Mod\u00e8le Llama (Direct) : {answer_direct.strip()}\")\n\n        print(\"\\n--- R\u00e9ponse du RAG (Bas\u00e9e sur le Contexte du Fichier) ---\")\n        response_rag = retrieval_chain.invoke({\"input\": transformed_query})\n        print(f\"Mod\u00e8le RAG : {response_rag['answer'].strip()}\")\n\n    except KeyboardInterrupt:\n        print(\"\\nFermeture.\")\n        break\n    except Exception as e:\n        print(f\"Une erreur est survenue : {e}\")\n        break\n</code></pre> <p>-&gt; R\u00e9sultats :  </p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Intelligence-Artificielle/rag_llama_minio_blog/#conclusion","title":"Conclusion","text":"<p>Cet article vous a guid\u00e9 \u00e0 travers la mise en place d'un syst\u00e8me RAG performant, d\u00e9connect\u00e9 et enti\u00e8rement open-source. Vous avez appris \u00e0 pr\u00e9parer les mod\u00e8les, \u00e0 les centraliser sur MinIO, \u00e0 construire un pipeline d'ingestion de donn\u00e9es s\u00e9mantique et \u00e0 assembler une cha\u00eene de g\u00e9n\u00e9ration avanc\u00e9e. La cl\u00e9 est maintenant d'exp\u00e9rimenter avec vos propres documents, d'ajuster les param\u00e8tres du prompt ou m\u00eame de tester diff\u00e9rents retrievers pour affiner encore plus les performances de votre assistant IA. Dans cet exemple, j\u2019ai utilis\u00e9 le mod\u00e8le Llama 3B de Meta-Llama, qui reste relativement l\u00e9ger. Bien qu\u2019il permette d\u2019illustrer le fonctionnement g\u00e9n\u00e9ral du syst\u00e8me, ses capacit\u00e9s sont limit\u00e9es et ne permettent pas toujours d\u2019obtenir des r\u00e9ponses puissantes ou tr\u00e8s d\u00e9taill\u00e9es. En utilisant un mod\u00e8le plus performant (comme Llama 7B, Mistral 7B ou Mixtral), vous pourrez obtenir des r\u00e9sultats bien plus pertinents et qualitatifs.</p> <p>Auteur : Mohamed-Reda BOUAMOUD</p>","tags":["OpenShift","IA","RAG","LLM","Llama 3","LangChain","MinIO"]},{"location":"OpenShift/Operator/Vaultwarden/","title":"Cr\u00e9ation d'un Operator sur OpenShift avec Vaultwarden","text":"","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#introduction","title":"Introduction","text":"<p>Les Operators sont devenus un \u00e9l\u00e9ment essentiel dans l'\u00e9cosyst\u00e8me Kubernetes, permettant d'automatiser les t\u00e2ches de gestion des applications. Dans cet article, nous allons cr\u00e9er un Operator pour Vaultwarden, une impl\u00e9mentation en Rust de Bitwarden, et le d\u00e9ployer sur OpenShift. Pour cela, nous utiliserons l'Operator SDK avec le plugin Helm.</p> <p></p>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#quest-ce-que-vaultwarden","title":"Qu'est-ce que Vaultwarden ?","text":"<p>Vaultwarden est une alternative open-source \u00e0 Bitwarden, un gestionnaire de mots de passe populaire. Il permet aux utilisateurs d'auto-h\u00e9berger leur propre serveur de gestion de mots de passe, offrant ainsi un contr\u00f4le total sur leurs donn\u00e9es sensibles.</p>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#prerequis","title":"Pr\u00e9requis","text":"","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#installer-loperator-sdk","title":"Installer l'Operator SDK","text":"<p>Pour cr\u00e9er notre Operator, nous devons d'abord installer l'Operator SDK. Voici comment proc\u00e9der :</p>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#telecharger-et-installer-loperator-sdk","title":"T\u00e9l\u00e9charger et installer l'Operator SDK :","text":"<pre><code>curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v1.25.0/operator-sdk_linux_amd64\nchmod +x operator-sdk_linux_amd64\nsudo mv operator-sdk_linux_amd64 /usr/local/bin/operator-sdk\n</code></pre>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#verifier-linstallation","title":"V\u00e9rifier l'installation :","text":"<pre><code>operator-sdk version\n</code></pre>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#droits-administrateurs-sur-le-cluster-openshift","title":"Droits Administrateurs sur le Cluster OpenShift","text":"<p>Assurez-vous d'avoir les droits administrateurs n\u00e9cessaires sur votre cluster OpenShift pour cr\u00e9er et g\u00e9rer des Operators.</p>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#etapes-de-creation-de-loperator","title":"\u00c9tapes de Cr\u00e9ation de l'Operator","text":"","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#initialiser-le-projet","title":"Initialiser le Projet","text":"<p>Utilisez l'Operator SDK pour initialiser un nouveau projet d'Operator avec le plugin Helm.</p> <pre><code>operator-sdk init --domain neutron-it --plugins helm --group vaultwarden --version v2alpha1 --kind VaultwardenApp --helm-chart=../vaultwarden-helm\n</code></pre>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#configurer-les-roles-rbac","title":"Configurer les R\u00f4les RBAC","text":"<p>Modifiez le fichier <code>config/rbac/role.yaml</code> pour accorder les droits n\u00e9cessaires \u00e0 l'Operator. Dans notre cas, nous allons donner des droits administrateurs au <code>manager-role</code>.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: manager-role\nrules:\n  - apiGroups:\n      - \"*\"\n    resources:\n      - \"*\"\n    verbs:\n      - \"*\"\n</code></pre>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#ajuster-les-ressources-du-manager","title":"Ajuster les Ressources du Manager","text":"<p>Dans le fichier <code>config/manager/manager.yaml</code>, augmentez la limite de m\u00e9moire pour le Manager.</p> <pre><code>resources:\n  limits:\n    memory: 1024Mi\n</code></pre>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#construire-et-pousser-limage-docker","title":"Construire et Pousser l'Image Docker","text":"<p>Construisez l'image Docker de l'Operator et poussez-la vers votre registre.</p> <pre><code>make docker-build docker-push IMG=\"quay.io/neutron-it/vaultwarden-operator:v0.0.6\"\n</code></pre>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#creer-et-pousser-le-bundle","title":"Cr\u00e9er et Pousser le Bundle","text":"<p>Cr\u00e9ez le bundle de l'Operator et poussez-le vers le registre.</p> <pre><code>make bundle IMG=\"quay.io/neutron-it/vaultwarden-operator:v0.0.6\"\nmake bundle-build bundle-push BUNDLE_IMG=\"quay.io/neutron-it/vaultwarden-operator-bundle:v0.0.6\" IMG=\"quay.io/neutron-it/vaultwarden-operator:v0.0.6\"\n</code></pre>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#executer-le-bundle","title":"Ex\u00e9cuter le Bundle","text":"<p>Ex\u00e9cutez le bundle pour installer l'Operator sur votre cluster OpenShift.</p> <pre><code>operator-sdk run bundle \"quay.io/neutron-it/vaultwarden-operator-bundle:v0.0.6\"\n</code></pre>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#deploiement-de-vaultwardenapp","title":"D\u00e9ploiement de VaultwardenApp","text":"","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#accedez-a-linterface-utilisateur-dopenshift","title":"Acc\u00e9dez \u00e0 l'interface utilisateur d'OpenShift.","text":"","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#allez-dans-la-section-installed-operators","title":"Allez dans la section \"Installed Operators\".","text":"","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#creez-une-instance-de-vaultwardenapp-avec-la-configuration-suivante","title":"Cr\u00e9ez une instance de <code>VaultwardenApp</code> avec la configuration suivante :","text":"<pre><code>apiVersion: vaultwarden.neutron-it/v2alpha1\nkind: VaultwardenApp\nmetadata:\n  name: vaultwardenapp-sample\n  namespace: neutron-it-ns\nspec:\n  config:\n    DATA_FOLDER: /data\n    INVITATIONS_ALLOWED: 'true'\n    ROCKET_PORT: '8080'\n    ROCKET_WORKERS: '10'\n    SHOW_PASSWORD_HINT: 'false'\n    SIGNUPS_ALLOWED: 'true'\n    SMTP_FROM: demonstration@yourdomain.fr\n    SMTP_HOST: aspmx.l.google.com\n    SMTP_PORT: '25'\n    SMTP_SECURITY: 'off'\n    WEBSOCKET_ENABLED: 'true'\n    WEB_VAULT_ENABLED: 'true'\n    YUBICO_CLIENT_ID: '123123'\n    YUBICO_SECRET_KEY: 12345678\n  consoleLink:\n    enabled: true\n    imageURL: 'https://upload.wikimedia.org/wikipedia/commons/8/83/Vaultwarden_icon.svg'\n    section: secret console link\n    text: Vaultwarden\n  domain: 'https://vaultwarden.apps.neutron-sno-office.intraneutron.fr'\n  image:\n    pullPolicy: IfNotPresent\n    repository: vaultwarden/server\n    tag: latest\n  persistence:\n    accessModes:\n      - ReadWriteOnce\n    enabled: true\n    size: 10Gi\n  rbac:\n    create: true\n  replicaCount: 1\n  resources: {}\n  route:\n    enabled: true\n    tls:\n      termination: edge\n    wildcardPolicy: None\n  service:\n    ports:\n      - name: http\n        port: 80\n        targetPort: 8080\n      - name: websocket\n        port: 3012\n        targetPort: 3012\n    type: ClusterIP\n  serviceAccount:\n    create: true\n    name: vaultwarden\n</code></pre> <p>Maintenant, vous pouvez vous rendre dans la section \"Console Links\" de l'interface OpenShift, cliquer sur \"Vaultwarden\" et acc\u00e9der directement \u00e0 l'interface de Vaultwarden pour commencer \u00e0 l'utiliser.</p> <p></p>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/Operator/Vaultwarden/#conclusion","title":"Conclusion","text":"<p>En suivant ces \u00e9tapes, vous avez cr\u00e9\u00e9 et d\u00e9ploy\u00e9 un Operator pour Vaultwarden sur OpenShift. Cet Operator automatise la gestion de Vaultwarden, facilitant ainsi le d\u00e9ploiement et la maintenance de l'application. Vous pouvez maintenant profiter des fonctionnalit\u00e9s de Vaultwarden tout en b\u00e9n\u00e9ficiant de l'automatisation offerte par les Operators Kubernetes.</p> <p>Auteur : Florian EVEN</p>","tags":["OpenShift","Operator","OpenShift","Operator","Vaultwarden"]},{"location":"OpenShift/RealTime/RealTime/","title":"Utiliser les capacit\u00e9s temps r\u00e9el d'OpenShift","text":"<p>Face \u00e0 l'essor des applications \u00e0 contraintes temps r\u00e9el dans les environnements industriels et financiers, les entreprises doivent repenser leurs infrastructures pour offrir \u00e0 la fois flexibilit\u00e9, faible latence et robustesse. OpenShift, la plateforme de Red Hat bas\u00e9e sur Kubernetes, propose des capacit\u00e9s temps r\u00e9el pour des workloads containers et machines virtuelles, tout en garantissant la s\u00e9curit\u00e9 de ceux-ci.  Cet article explore comment configurer une infrastructure mixte - containers et VMs - optimis\u00e9e pour les contraintes temps r\u00e9el.</p>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#contexte-et-enjeux-des-contraintes-temps-reel-sur-openshift","title":"Contexte et enjeux des contraintes temps r\u00e9el sur OpenShift","text":"","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#les-defis-des-workloads-temps-reel","title":"Les d\u00e9fis des workloads temps r\u00e9el","text":"<p>Les applications temps r\u00e9el exigent un traitement des donn\u00e9es et une r\u00e9ponse aux \u00e9v\u00e9nements dans des d\u00e9lais extr\u00eamement courts et stables. Parmi les enjeux principaux, on retrouve : - La pr\u00e9visibilit\u00e9 du temps de r\u00e9ponse : L'isolation des ressources (CPU, m\u00e9moire et E/S) est indispensable pour \u00e9viter les interf\u00e9rences. - La latence r\u00e9seau minimale : Pour les communications critiques, il est souvent n\u00e9cessaire d'utiliser des interfaces r\u00e9seau qui contournent les couches de virtualisation classiques. - La gestion simultan\u00e9e de containers et de VMs : Certains workloads h\u00e9rit\u00e9s fonctionnant en VM doivent cohabiter avec des applications nativement containeris\u00e9es, sans compromettre la performance globale.  </p>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#openshift-comme-plateforme-unifiee","title":"OpenShift comme plateforme unifi\u00e9e","text":"<p>OpenShift permet de r\u00e9unir ces deux types de workloads \u00e0 l'aide de son op\u00e9rateur OpenShift Virtualization. Ainsi, il devient possible de g\u00e9rer dans une m\u00eame plateforme des applications containeris\u00e9es ultra r\u00e9actives et des VMs pour des charges traditionnelles. Cette approche facilite notamment la migration progressive des applications vers des architectures modernes tout en garantissant un temps de r\u00e9ponse optimal.</p>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#integration-de-containers-et-vms-pour-des-workloads-temps-reel","title":"Int\u00e9gration de containers et VMs pour des workloads temps r\u00e9el","text":"","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#configuration-dopenshift","title":"Configuration d'OpenShift","text":"<p>Pour ex\u00e9cuter des workloads temps r\u00e9el, OpenShift doit \u00eatre configur\u00e9 avec : - Un noyau temps r\u00e9el : Disponible via l'op\u00e9rateur OpenShift RT, ce noyau permet d'am\u00e9liorer la stabilit\u00e9 des temps de r\u00e9ponse. - L'isolation des CPUs : En d\u00e9diant des c\u0153urs sp\u00e9cifiques aux t\u00e2ches critiques, on r\u00e9duit la variabilit\u00e9 des performances. - L'optimisation des interruptions : Ajuster les IRQs et configurer l'affinit\u00e9 CPU permet d'\u00e9viter les interruptions non d\u00e9sir\u00e9es. - L'utilisation de HugePages : Cela optimise l'utilisation m\u00e9moire et am\u00e9liore la gestion des latences.  Tout ceci peut \u00eatre configur\u00e9 \u00e0 l'aide d'un <code>PerformanceProfile</code>.</p> <p></p>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#configuration-dun-performanceprofile","title":"Configuration d'un PerformanceProfile","text":"<p>L'activation des performances temps r\u00e9el passe par la cr\u00e9ation d'un <code>PerformanceProfile</code> d\u00e9di\u00e9 :</p> <pre><code>---\napiVersion: performance.openshift.io/v2\nkind: PerformanceProfile\nmetadata:\n  name: rt-profile\nspec:\n  cpu:\n    isolated: 2-7\n    reserved: 0-1\n  hugepages:\n    defaultHugepagesSize: 1G\n    pages:\n      - count: 4\n        size: 1G\n  realTimeKernel:\n    enabled: true\n  workloadHints:\n    realTime: true\n  nodeSelector:\n    node-role.kubernetes.io/worker-rt: \"\"\n</code></pre>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#deploiement-dun-container-temps-reel","title":"D\u00e9ploiement d'un container temps r\u00e9el","text":"<p>Pour d\u00e9ployer un container temps r\u00e9el sur OpenShift en QoS guaranteed :</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: rt-container\nspec:\n  containers:\n  - name: app-rt\n    image: registry.example.com/rt-app:latest\n    resources:\n      requests:\n        cpu: \"2\"\n        memory: \"2Gi\"\n        hugepages-1Gi: \"2Gi\"\n      limits:\n        cpu: \"2\"\n        memory: \"2Gi\"\n        hugepages-1Gi: \"2Gi\"\n    securityContext:\n      capabilities:\n        add:\n        - SYS_NICE # Allows the container to adjust scheduling priorities.\n        - IPC_LOCK # Enables locking memory to prevent swapping (used in real-time applications).\n</code></pre>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#explication-des-qos-dans-openshift","title":"Explication des QoS dans OpenShift","text":"<p><code>guaranteed</code>: Cette classe garantit l'allocation des ressources, elle est d\u00e9finie lorsqu'on sp\u00e9cifie <code>requests</code>\u00e9gal \u00e0 <code>limits</code>.</p> <p><code>burstable</code> : Cette classe permet d'allouer plus de ressources si celles-ci sont disponibles, elle est d\u00e9finie lorsqu'on sp\u00e9cifie <code>limits</code> sup\u00e9rieur \u00e0 <code>requests</code>.</p> <p><code>bestEffort</code> : Cette classe utilise les ressources disponibles dynamiquement et peut \u00eatre expuls\u00e9 si le n\u0153ud manque de ressources, elle est d\u00e9finie lorsqu'on ne sp\u00e9cifie aucune <code>requests</code> et aucune <code>limits</code>.</p>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#deploiement-dune-vm-temps-reel","title":"D\u00e9ploiement d'une VM temps r\u00e9el","text":"<p>Pour cr\u00e9er une VM temps r\u00e9el avec OpenShift Virtualization :</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: rt-vm\nspec:\n  running: true\n  template:\n    spec:\n      domain:\n        cpu:\n          model: \"host-passthrough\" # Uses the host CPU model for optimal performance.\n          cores: 2 # Number of virtual CPU cores allocated to the VM.\n          dedicatedCpuPlacement: true # Ensures CPU cores are dedicated for the VM.\n          isolateEmulatorThread: true # Isolates the emulator thread for better latency.\n        memory:\n          hugepages:\n            pageSize: \"1Gi\"\n        resources:\n          requests:\n            memory: \"4Gi\"\n          limits:\n            memory: \"4Gi\"\n        devices:\n          disks:\n            - name: rootdisk\n              disk:\n                bus: virtio\n          interfaces:\n            - name: default\n              bridge: {}\n      networks:\n        - name: default\n          pod: {}\n  dataVolumeTemplates:\n    - metadata:\n        name: rt-vm-disk\n      spec:\n        source:\n          http:\n            url: \"http://example.com/rt-vm.qcow2\"\n        pvc:\n          accessModes:\n            - ReadWriteOnce\n          resources:\n            requests:\n              storage: 10Gi\n</code></pre>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#explication-des-parametres-cles-des-vms-temps-reel","title":"Explication des param\u00e8tres cl\u00e9s des VMs temps r\u00e9el","text":"<p><code>host-passthrough</code> : Ce param\u00e8tre permet de passer directement le mod\u00e8le de CPU de l'h\u00f4te \u00e0 la VM, garantissant ainsi une compatibilit\u00e9 et une performance maximales. Il \u00e9vite l'\u00e9mulation de certaines instructions CPU, r\u00e9duisant ainsi la latence et augmentant la stabilit\u00e9 du temps de r\u00e9ponse.</p> <p><code>dedicatedCpuPlacement</code> : L'activation de cette option garantit que les c\u0153urs CPU allou\u00e9s \u00e0 la VM ne seront pas partag\u00e9s avec d'autres workloads, am\u00e9liorant ainsi la pr\u00e9visibilit\u00e9 des performances et minimisant l'interf\u00e9rence avec d'autres processus ex\u00e9cut\u00e9s sur le n\u0153ud.</p> <p><code>isolateEmulatorThread</code> : L'activation de ce param\u00e8tre permet d'isoler le thread de l'\u00e9mulateur, ce qui r\u00e9duit la latence de l'\u00e9mulation et am\u00e9liore les performances en cas de workloads n\u00e9cessitant un traitement rapide et d\u00e9terministe.</p>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#utilisation-dune-interface-reseau-l2","title":"Utilisation d'une interface r\u00e9seau L2","text":"<p>Les applications temps r\u00e9el requi\u00e8rent souvent une communication r\u00e9seau \u00e0 faible latence et haut d\u00e9bit. L'utilisation d'une interface r\u00e9seau L2 permet de r\u00e9duire la surcharge introduite par les couches de virtualisation r\u00e9seau classiques et d'offrir une connectivit\u00e9 directe entre les workloads.   </p> <p>OpenShift permet d'exploiter Multus pour cr\u00e9er des interfaces L2 en utilisant macvlan. </p>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#creation-dun-nad-multus-en-macvlan","title":"Cr\u00e9ation d'un NAD Multus en macvlan","text":"<pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: l2-interface\n  namespace: example-namespace\nspec:\n  config: '{\n    \"cniVersion\": \"0.3.1\",\n    \"type\": \"macvlan\",\n    \"master\": \"enp3s0\",\n    \"mode\": \"bridge\",\n    \"ipam\": { \"type\": \"dhcp\" }\n  }'\n</code></pre>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#association-dune-interface-l2-a-un-container","title":"Association d'une interface L2 \u00e0 un container","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: rt-container-l2\n  annotations:\n    k8s.v1.cni.cncf.io/networks: example-namespace/l2-interface\nspec:\n  containers:\n  - name: rt-app\n    image: registry.example.com/rt-app:latest\n</code></pre>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#association-dune-interface-l2-a-une-vm","title":"Association d'une interface L2 \u00e0 une VM","text":"<pre><code>spec:\n  template:\n    spec:\n      networks:\n        - name: l2-net\n          multus:\n            networkName: example-namespace/l2-interface\n      domain:\n        devices:\n          interfaces:\n            - name: l2-net\n              bridge: {}\n</code></pre>","tags":["OpenShift","RealTime"]},{"location":"OpenShift/RealTime/RealTime/#conclusion","title":"Conclusion","text":"<p>OpenShift offre une plateforme unifi\u00e9e pour ex\u00e9cuter des workloads temps r\u00e9el en combinant containers et machines virtuelles. Gr\u00e2ce \u00e0 ses capacit\u00e9s d'optimisation des performances, de gestion fine des ressources et de mise en r\u00e9seau avanc\u00e9e, il r\u00e9pond aux exigences des environnements industriels et financiers critiques. En configurant correctement les ressources CPU, m\u00e9moire et r\u00e9seau, il est possible d'atteindre une faible latence et une grande pr\u00e9visibilit\u00e9, garantissant ainsi la stabilit\u00e9 et la performance des applications temps r\u00e9el.</p> <p>Auteur : Romain GASQUET</p>","tags":["OpenShift","RealTime"]}]}